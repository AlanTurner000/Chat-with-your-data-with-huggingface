{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13680\\3853144719.py:5: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "#using llm on huggingface not openai api\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 26\n",
    "chunk_overlap = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'm n o p q r s t u v w x y', 'y z']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=2,\n",
    "    separator = ' '\n",
    ")\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive splitting details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 450,\n",
    "    chunk_overlap = 0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 450,\n",
    "    chunk_overlap = 0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Lecture Notes.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 150,\n",
    "    separator='\\n',\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is neural network? \n",
      "It is a powerful learning algorithm inspired by how the brain works. \n",
      "Example 1 – single neural network \n",
      "Given data about the size of ho uses on the real estate market and you want to fit a function that will  \n",
      "predict their price. It is a linear regression problem because the price as a function of size is a continuous \n",
      "output. \n",
      "We know the prices can never be negative so we are creating a function called Rectified Linear Unit (ReLU) \n",
      "which starts at zero. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "The input is the size of the house (x) \n",
      "The output is the price (y) \n",
      "The “neuron” implements the function ReLU (blue line) \n",
      "Example 2 – Multiple neural network \n",
      "The price of a house  can be affected by other features such as size, number of bedrooms, zip code and \n",
      "wealth. The role of the neural network is to predicted the price and it will automatically generate the \n",
      "hidden units. We only need to give the inputs x and the output y.  \n",
      " \n",
      "Input layer Hidden unit\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "# from langchain_community.document_loaders.notion import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs\")\n",
    "notion_db = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(notion_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notion_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='# Neural Networks and Deep Learning\\nThis is the first course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\\n## Table of contents\\n* [Neural Networks and Deep Learning](#neural-networks-and-deep-learning)\\n   * [Table of contents](#table-of-contents)\\n   * [Course summary](#course-summary)\\n   * [Introduction to deep learning](#introduction-to-deep-learning)\\n      * [What is a (Neural Network) NN?](#what-is-a-neural-network-nn)\\n      * [Supervised learning with neural networks](#supervised-learning-with-neural-networks)\\n      * [Why is deep learning taking off?](#why-is-deep-learning-taking-off)\\n   * [Neural Networks Basics](#neural-networks-basics)\\n      * [Binary classification](#binary-classification)\\n      * [Logistic regression](#logistic-regression)\\n      * [Logistic regression cost function](#logistic-regression-cost-function)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='* [Logistic regression](#logistic-regression)\\n      * [Logistic regression cost function](#logistic-regression-cost-function)\\n      * [Gradient Descent](#gradient-descent)\\n      * [Derivatives](#derivatives)\\n      * [More Derivatives examples](#more-derivatives-examples)\\n      * [Computation graph](#computation-graph)\\n      * [Derivatives with a Computation Graph](#derivatives-with-a-computation-graph)\\n      * [Logistic Regression Gradient Descent](#logistic-regression-gradient-descent)\\n      * [Gradient Descent on m Examples](#gradient-descent-on-m-examples)\\n      * [Vectorization](#vectorization)\\n      * [Vectorizing Logistic Regression](#vectorizing-logistic-regression)\\n      * [Notes on Python and NumPy](#notes-on-python-and-numpy)\\n      * [General Notes](#general-notes)\\n   * [Shallow neural networks](#shallow-neural-networks)\\n      * [Neural Networks Overview](#neural-networks-overview)\\n      * [Neural Network Representation](#neural-network-representation)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"* [Neural Networks Overview](#neural-networks-overview)\\n      * [Neural Network Representation](#neural-network-representation)\\n      * [Computing a Neural Network's Output](#computing-a-neural-networks-output)\\n      * [Vectorizing across multiple examples](#vectorizing-across-multiple-examples)\\n      * [Activation functions](#activation-functions)\\n      * [Why do you need non-linear activation functions?](#why-do-you-need-non-linear-activation-functions)\\n      * [Derivatives of activation functions](#derivatives-of-activation-functions)\\n      * [Gradient descent for Neural Networks](#gradient-descent-for-neural-networks)\\n      * [Random Initialization](#random-initialization)\\n   * [Deep Neural Networks](#deep-neural-networks)\\n      * [Deep L-layer neural network](#deep-l-layer-neural-network)\\n      * [Forward Propagation in a Deep Network](#forward-propagation-in-a-deep-network)\\n      * [Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='* [Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)\\n      * [Why deep representations?](#why-deep-representations)\\n      * [Building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)\\n      * [Forward and Backward Propagation](#forward-and-backward-propagation)\\n      * [Parameters vs Hyperparameters](#parameters-vs-hyperparameters)\\n      * [What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)\\n   * [Extra: Ian Goodfellow interview](#extra-ian-goodfellow-interview)\\n## Course summary\\nHere are the course summary as its given on the course [link](https://www.coursera.org/learn/neural-networks-deep-learning):'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='## Course summary\\nHere are the course summary as its given on the course [link](https://www.coursera.org/learn/neural-networks-deep-learning):\\n> If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new \"superpower\" that will let you build AI systems that just weren\\'t possible a few years ago.\\n>\\n> In this course, you will learn the foundations of deep learning. When you finish this class, you will:\\n> - Understand the major technology trends driving Deep Learning\\n> - Be able to build, train and apply fully connected deep neural networks\\n> - Know how to implement efficient (vectorized) neural networks\\n> - Understand the key parameters in a neural network\\'s architecture\\n>'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"> - Know how to implement efficient (vectorized) neural networks\\n> - Understand the key parameters in a neural network's architecture\\n>\\n> This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions.\\n## Introduction to deep learning\\n> Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.\\n### What is a (Neural Network) NN?\\n- Single neuron == linear regression without applying activation(perceptron)\\n- Basically a single neuron will calculate weighted sum of input(W.T*X) and then we can set a threshold to predict output in a perceptron. If weighted sum of input cross the threshold, perceptron fires and if not then perceptron doesn't predict.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Perceptron can take real values input or boolean values.\\n- Actually, when w⋅x+b=0 the perceptron outputs 0.\\n- Disadvantage of perceptron is that it only output binary values and if we try to give small change in weight and bais then perceptron can flip the output. We need some system which can modify the output slightly according to small change in weight and bias. Here comes sigmoid function in picture.\\n- If we change perceptron with a sigmoid function, then we can make slight change in output.\\n- e.g. output in perceptron = 0, you slightly changed weight and bias, output becomes = 1 but actual output is 0.7. In case of sigmoid, output1 = 0, slight change in weight and bias, output = 0.7. \\n- If we apply sigmoid activation function then Single neuron will act as Logistic Regression.\\n-  we can understand difference between perceptron and sigmoid function by looking at sigmoid function graph.\\n- Simple NN graph:\\n  - ![](Images/Others/01.jpg)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Simple NN graph:\\n  - ![](Images/Others/01.jpg)\\n  - Image taken from [tutorialspoint.com](http://www.tutorialspoint.com/)\\n- RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.\\n- Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.\\n- Deep NN consists of more hidden layers (Deeper layers)\\n  - ![](Images/Others/02.png)\\n  - Image taken from [opennn.net](http://www.opennn.net/)\\n- Each Input will be connected to the hidden layer and the NN will decide the connections.\\n- Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.\\n### Supervised learning with neural networks\\n- Different types of neural networks for supervised learning which includes:\\n  - CNN or convolutional neural networks (Useful in computer vision)\\n  - RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\\n  - Standard NN (Useful for Structured data)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\\n  - Standard NN (Useful for Structured data)\\n  - Hybrid/custom NN or a Collection of NNs types\\n- Structured data is like the databases and tables.\\n- Unstructured data is like images, video, audio, and text.\\n- Structured data gives more money because companies relies on prediction on its big data.\\n### Why is deep learning taking off?\\n- Deep learning is taking off for 3 reasons:\\n  1. Data:\\n     - Using this image we can conclude:\\n       - ![](Images/11.png)\\n     - For small data NN can perform as Linear regression or SVM (Support vector machine)\\n     - For big data a small NN is better that SVM\\n     - For big data a big NN is better that a medium NN is better that small NN.\\n     - Hopefully we have a lot of data because the world is using the computer a little bit more\\n       - Mobiles\\n       - IOT (Internet of things)\\n  2. Computation:\\n     - GPUs.\\n     - Powerful CPUs.\\n     - Distributed computing.\\n     - ASICs'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Mobiles\\n       - IOT (Internet of things)\\n  2. Computation:\\n     - GPUs.\\n     - Powerful CPUs.\\n     - Distributed computing.\\n     - ASICs\\n  3. Algorithm:\\n     1. Creative algorithms has appeared that changed the way NN works.\\n        - For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem.\\n  \\u200b\\n## Neural Networks Basics\\n> Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.\\n### Binary classification\\n- Mainly he is talking about how to do a logistic regression to make a binary classifier.\\n  - ![log](Images/Others/03.png)\\n  - Image taken from [3.bp.blogspot.com](http://3.bp.blogspot.com)\\n- He talked about an example of knowing if the current image contains a cat or not.\\n- Here are some notations:\\n  - `M is the number of training vectors`\\n  - `Nx is the size of the input vector`'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- Here are some notations:\\n  - `M is the number of training vectors`\\n  - `Nx is the size of the input vector`\\n  - `Ny is the size of the output vector`\\n  - `X(1) is the first input vector`\\n  - `Y(1) is the first output vector`\\n  - `X = [x(1) x(2).. x(M)]`\\n  - `Y = (y(1) y(2).. y(M))`\\n- We will use python in this course.\\n- In NumPy we can make matrices and make operations on them in a fast and reliable time.\\n### Logistic regression\\n- Algorithm is used for classification algorithm of 2 classes.\\n- Equations:\\n  - Simple equation:\\t`y = wx + b`\\n  - If x is a vector: `y = w(transpose)x + b`\\n  - If we need y to be in between 0 and 1 (probability): `y = sigmoid(w(transpose)x + b)`\\n  - In some notations this might be used: `y = sigmoid(w(transpose)x)`\\n    - While `b` is `w0` of `w` and we add `x0 = 1`. but we won't use this notation in the course (Andrew said that the first notation is better).\\n- In binary classification `Y` has to be between `0` and `1`.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- In binary classification `Y` has to be between `0` and `1`.\\n- In the last equation `w` is a vector of `Nx` and `b` is a real number\\n### Logistic regression cost function\\n- First loss function would be the square root error:  `L(y',y) = 1/2 (y' - y)^2`\\n  - But we won't use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.\\n- This is the function that we will use: `L(y',y) = - (y*log(y') + (1-y)*log(1-y'))`\\n- To explain the last function lets see:\\n  - if `y = 1` ==> `L(y',1) = -log(y')`  ==> we want `y'` to be the largest   ==> `y`' biggest value is 1\\n  - if `y = 0` ==> `L(y',0) = -log(1-y')` ==> we want `1-y'` to be the largest ==> `y'` to be smaller as possible because it can only has 1 value.\\n- Then the Cost function will be: `J(w,b) = (1/m) * Sum(L(y'[i],y[i]))`\\n- The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='### Gradient Descent\\n- We want to predict `w` and `b` that minimize the cost function.\\n- Our cost function is convex.\\n- First we initialize `w` and `b` to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.\\n- In Logistic regression people always use 0,0 instead of random.\\n- The gradient decent algorithm repeats: `w = w - alpha * dw`\\n  where alpha is the learning rate and `dw` is the derivative of `w` (Change to `w`)\\n  The derivative is also the slope of `w`\\n- Looks like greedy algorithms. the derivative give us the direction to improve our parameters.\\n- The actual equations we will implement:\\n  - `w = w - alpha * d(J(w,b) / dw)`        (how much the function slopes in the w direction)\\n  - `b = b - alpha * d(J(w,b) / db)`        (how much the function slopes in the d direction)\\n### Derivatives\\n- We will talk about some of required calculus.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"### Derivatives\\n- We will talk about some of required calculus.\\n- You don't need to be a calculus geek to master deep learning but you'll need some skills from it.\\n- Derivative of a linear line is its slope.\\n  - ex. `f(a) = 3a`                    `d(f(a))/d(a) = 3`\\n  - if `a = 2` then `f(a) = 6`\\n  - if we move a a little bit `a = 2.001` then `f(a) = 6.003` means that we multiplied the derivative (Slope) to the moved area and added it to the last result.\\n### More Derivatives examples\\n- `f(a) = a^2`  ==> `d(f(a))/d(a) = 2a`\\n  - `a = 2`  ==> `f(a) = 4`\\n  - `a = 2.0001` ==> `f(a) = 4.0004` approx.\\n- `f(a) = a^3`  ==> `d(f(a))/d(a) = 3a^2`\\n- `f(a) = log(a)`  ==> `d(f(a))/d(a) = 1/a`\\n- To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.\\n### Computation graph\\n- Its a graph that organizes the computation from left to right.\\n  - ![](Images/02.png)\\n### Derivatives with a Computation Graph\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='### Computation graph\\n- Its a graph that organizes the computation from left to right.\\n  - ![](Images/02.png)\\n### Derivatives with a Computation Graph\\n- Calculus chain rule says:\\n  If `x -> y -> z`          (x effect y and y effects z)\\n  Then `d(z)/d(x) = d(z)/d(y) * d(y)/d(x)`\\n- The video illustrates a big example.\\n  - ![](Images/03.png)\\n- We compute the derivatives on a graph from right to left and it will be a lot more easier.\\n- `dvar` means the derivatives of a final output variable with respect to various intermediate quantities.\\n### Logistic Regression Gradient Descent\\n- In the video he discussed the derivatives of gradient decent example for one sample with two features `x1` and `x2`.\\n  - ![](Images/04.png)\\n### Gradient Descent on m Examples\\n- Lets say we have these variables:\\n  ```\\n  \\tX1                  Feature\\n  \\tX2                  Feature\\n  \\tW1                  Weight of the first feature.\\n  \\tW2                  Weight of the second feature.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='X2                  Feature\\n  \\tW1                  Weight of the first feature.\\n  \\tW2                  Weight of the second feature.\\n  \\tB                   Logistic Regression parameter.\\n  \\tM                   Number of training examples\\n  \\tY(i)                Expected output of i\\n  ```\\n- So we have:\\n  ![](Images/09.png)\\n- Then from right to left we will calculate derivations compared to the result:\\n  ```\\n  \\td(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))\\n  \\td(z)  = d(l)/d(z) = a - y\\n  \\td(W1) = X1 * d(z)\\n  \\td(W2) = X2 * d(z)\\n  \\td(B)  = d(z)\\n  ```\\n- From the above we can conclude the logistic regression pseudo code:\\n  ```\\n  \\tJ = 0; dw1 = 0; dw2 =0; db = 0;                 # Devs.\\n  \\tw1 = 0; w2 = 0; b=0;\\t\\t\\t\\t\\t\\t\\t# Weights\\n  \\tfor i = 1 to m\\n  \\t\\t# Forward pass\\n  \\t\\tz(i) = W1*x1(i) + W2*x2(i) + b\\n  \\t\\ta(i) = Sigmoid(z(i))\\n  \\t\\tJ += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))\\n  \\t\\t# Backward pass\\n  \\t\\tdz(i) = a(i) - Y(i)\\n  \\t\\tdw1 += dz(i) * x1(i)\\n  \\t\\tdw2 += dz(i) * x2(i)\\n  \\t\\tdb  += dz(i)\\n  \\tJ /= m'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='# Backward pass\\n  \\t\\tdz(i) = a(i) - Y(i)\\n  \\t\\tdw1 += dz(i) * x1(i)\\n  \\t\\tdw2 += dz(i) * x2(i)\\n  \\t\\tdb  += dz(i)\\n  \\tJ /= m\\n  \\tdw1/= m\\n  \\tdw2/= m\\n  \\tdb/= m\\n  \\t# Gradient descent\\n  \\tw1 = w1 - alpha * dw1\\n  \\tw2 = w2 - alpha * dw2\\n  \\tb = b - alpha * db\\n  ```\\n- The above code should run for some iterations to minimize error.\\n- So there will be two inner loops to implement the logistic regression.\\n- Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!\\n### Vectorization\\n- Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why we need vectorization to get rid of some of our for loops.\\n- NumPy library (dot) function is using vectorization by default.\\n- The vectorization can be done on CPU or GPU thought the SIMD operation. But its faster on GPU.\\n- Whenever possible avoid for loops.\\n- Most of the NumPy library methods are vectorized version.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- Whenever possible avoid for loops.\\n- Most of the NumPy library methods are vectorized version.\\n### Vectorizing Logistic Regression\\n- We will implement Logistic Regression using one for loop then without any for loop.\\n- As an input we have a matrix `X` and its `[Nx, m]` and a matrix `Y` and its `[Ny, m]`.\\n- We will then compute at instance `[z1,z2...zm] = W' * X + [b,b,...b]`. This can be written in python as:\\n    \\t\\tZ = np.dot(W.T,X) + b    # Vectorization, then broadcasting, Z shape is (1, m)\\n    \\t\\tA = 1 / 1 + np.exp(-Z)   # Vectorization, A shape is (1, m)\\n- Vectorizing Logistic Regression's Gradient Output:\\n   \\t\\t\\tdz = A - Y                  # Vectorization, dz shape is (1, m)\\n   \\t\\t\\tdw = np.dot(X, dz.T) / m    # Vectorization, dw shape is (Nx, 1)\\n   \\t\\t\\tdb = dz.sum() / m           # Vectorization, dz shape is (1, 1)\\n### Notes on Python and NumPy\\n- In NumPy, `obj.sum(axis = 0)` sums the columns while `obj.sum(axis = 1)` sums the rows.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"### Notes on Python and NumPy\\n- In NumPy, `obj.sum(axis = 0)` sums the columns while `obj.sum(axis = 1)` sums the rows.\\n- In NumPy, `obj.reshape(1,4)` changes the shape of the matrix by broadcasting the values.\\n- Reshape is cheap in calculations so put it everywhere you're not sure about the calculations.\\n- Broadcasting works when you do a matrix operation with matrices that doesn't match for the operation, in this case NumPy automatically makes the shapes ready for the operation by broadcasting the values.\\n- In general principle of broadcasting. If you have an (m,n) matrix and you add(+) or subtract(-) or multiply(*) or divide(/) with a (1,n) matrix, then this will copy it m times into an (m,n) matrix. The same with if you use those operations with a (m , 1) matrix, then this will copy it n times into (m, n) matrix. And then apply the addition, subtraction, and multiplication of division element wise.\\n- Some tricks to eliminate all the strange bugs in the code:\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- Some tricks to eliminate all the strange bugs in the code:\\n  - If you didn't specify the shape of a vector, it will take a shape of `(m,)` and the transpose operation won't work. You have to reshape it to `(m, 1)`\\n  - Try to not use the rank one matrix in ANN\\n  - Don't hesitate to use `assert(a.shape == (5,1))` to check if your matrix shape is the required one.\\n  - If you've found a rank one matrix try to run reshape on it.\\n- Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at the same time. It runs in the browser and doesn't need an IDE to run.\\n  - To open Jupyter Notebook, open the command line and call: `jupyter-notebook` It should be installed to work.\\n- To Compute the derivative of Sigmoid:\\n  ```\\n  \\ts = sigmoid(x)\\n  \\tds = s * (1 - s)       # derivative  using calculus\\n  ```\\n- To make an image of `(width,height,depth)` be a vector, use this:\\n  ```\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='ds = s * (1 - s)       # derivative  using calculus\\n  ```\\n- To make an image of `(width,height,depth)` be a vector, use this:\\n  ```\\n  v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)  #reshapes the image.\\n  ```\\n- Gradient descent converges faster after normalization of the input matrices.\\n### General Notes\\n- The main steps for building a Neural Network are:\\n  - Define the model structure (such as number of input features and outputs)\\n  - Initialize the model\\'s parameters.\\n  - Loop.\\n    - Calculate current loss (forward propagation)\\n    - Calculate current gradient (backward propagation)\\n    - Update parameters (gradient descent)\\n- Preprocessing the dataset is important.\\n- Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm.\\n- [kaggle.com](kaggle.com) is a good place for datasets and competitions.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- [kaggle.com](kaggle.com) is a good place for datasets and competitions.\\n- [Pieter Abbeel](https://www2.eecs.berkeley.edu/Faculty/Homepages/abbeel.html) is one of the best in deep reinforcement learning.\\n## Shallow neural networks\\n> Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.\\n### Neural Networks Overview\\n- In logistic regression we had:\\n  ```\\n  X1  \\\\  \\n  X2   ==>  z = XW + B ==> a = Sigmoid(z) ==> l(a,Y)\\n  X3  /\\n  ```\\n- In neural networks with one layer we will have:\\n  ```\\n  X1  \\\\  \\n  X2   =>  z1 = XW1 + B1 => a1 = Sigmoid(z1) => z2 = a1W2 + B2 => a2 = Sigmoid(z2) => l(a2,Y)\\n  X3  /\\n  ```\\n- `X` is the input vector `(X1, X2, X3)`, and `Y` is the output variable `(1x1)`\\n- NN is stack of logistic regression objects.\\n### Neural Network Representation\\n- We will define the neural networks that has one hidden layer.\\n- NN contains of input layers, hidden layers, output layers.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- We will define the neural networks that has one hidden layer.\\n- NN contains of input layers, hidden layers, output layers.\\n- Hidden layer means we cant see that layers in the training set.\\n- `a0 = x` (the input layer)\\n- `a1` will represent the activation of the hidden neurons.\\n- `a2` will represent the output layer.\\n- We are talking about 2 layers NN. The input layer isn't counted.\\n### Computing a Neural Network's Output\\n- Equations of Hidden layers:\\n  - ![](Images/05.png)\\n- Here are some informations about the last image:\\n  - `noOfHiddenNeurons = 4`\\n  - `Nx = 3`\\n  - Shapes of the variables:\\n    - `W1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,nx)`\\n    - `b1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,1)`\\n    - `z1` is the result of the equation `z1 = W1*X + b`, it has a shape of `(noOfHiddenNeurons,1)`\\n    - `a1` is the result of the equation `a1 = sigmoid(z1)`, it has a shape of `(noOfHiddenNeurons,1)`\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- `a1` is the result of the equation `a1 = sigmoid(z1)`, it has a shape of `(noOfHiddenNeurons,1)`\\n    - `W2` is the matrix of the second hidden layer, it has a shape of `(1,noOfHiddenNeurons)`\\n    - `b2` is the matrix of the second hidden layer, it has a shape of `(1,1)`\\n    - `z2` is the result of the equation `z2 = W2*a1 + b`, it has a shape of `(1,1)`\\n    - `a2` is the result of the equation `a2 = sigmoid(z2)`, it has a shape of `(1,1)`\\n### Vectorizing across multiple examples\\n- Pseudo code for forward propagation for the 2 layers NN:\\n  ```\\n  for i = 1 to m\\n    z[1, i] = W1*x[i] + b1      # shape of z[1, i] is (noOfHiddenNeurons,1)\\n    a[1, i] = sigmoid(z[1, i])  # shape of a[1, i] is (noOfHiddenNeurons,1)\\n    z[2, i] = W2*a[1, i] + b2   # shape of z[2, i] is (1,1)\\n    a[2, i] = sigmoid(z[2, i])  # shape of a[2, i] is (1,1)\\n  ```\\n- Lets say we have `X` on shape `(Nx,m)`. So the new pseudo code:\\n  ```\\n  Z1 = W1X + b1     # shape of Z1 (noOfHiddenNeurons,m)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='```\\n- Lets say we have `X` on shape `(Nx,m)`. So the new pseudo code:\\n  ```\\n  Z1 = W1X + b1     # shape of Z1 (noOfHiddenNeurons,m)\\n  A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\\n  Z2 = W2A1 + b2    # shape of Z2 is (1,m)\\n  A2 = sigmoid(Z2)  # shape of A2 is (1,m)\\n  ```\\n- If you notice always m is the number of columns.\\n- In the last example we can call `X` = `A0`. So the previous step can be rewritten as:\\n  ```\\n  Z1 = W1A0 + b1    # shape of Z1 (noOfHiddenNeurons,m)\\n  A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\\n  Z2 = W2A1 + b2    # shape of Z2 is (1,m)\\n  A2 = sigmoid(Z2)  # shape of A2 is (1,m)\\n  ```\\n### Activation functions\\n- So far we are using sigmoid, but in some cases other functions can be a lot better.\\n- Sigmoid can lead us to gradient decent problem where the updates are so low.\\n- Sigmoid activation function range is [0,1]\\n  `A = 1 / (1 + np.exp(-z)) # Where z is the input matrix`'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Sigmoid activation function range is [0,1]\\n  `A = 1 / (1 + np.exp(-z)) # Where z is the input matrix`\\n- Tanh activation function range is [-1,1]   (Shifted version of sigmoid function)\\n  - In NumPy we can implement Tanh using one of these methods:\\n    `A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) # Where z is the input matrix`\\n    Or\\n    `A = np.tanh(z)   # Where z is the input matrix`\\n- It turns out that the tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.\\n- Sigmoid or Tanh function disadvantage is that if the input is too small or too high, the slope will be near zero which will cause us the gradient decent problem.\\n- One of the popular activation functions that solved the slow gradient decent is the RELU function.\\n  `RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the slope remains linear.`'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='`RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the slope remains linear.`\\n- So here is some basic rule for choosing activation functions, if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.\\n- Leaky RELU activation function different of RELU is that if the input is negative the slope will be so small. It works as RELU but most people uses RELU.\\n  `Leaky_RELU = max(0.01z,z)  #the 0.01 can be a parameter for your algorithm.`\\n- In NN you will decide a lot of choices like:\\n  - No of hidden layers.\\n  - No of neurons in each hidden layer.\\n  - Learning rate.       (The most important parameter)\\n  - Activation functions.\\n  - And others..\\n- It turns out there are no guide lines for that. You should try all activation functions for example.\\n### Why do you need non-linear activation functions?\\n- If we removed the activation function from our algorithm that can be called linear activation function.'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- If we removed the activation function from our algorithm that can be called linear activation function.\\n- Linear activation function will output linear activations\\n  - Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems)\\n- You might use linear activation function in one place - in the output layer if the output is real numbers (regression problem). But even in this case if the output value is non-negative you could use RELU instead.\\n### Derivatives of activation functions\\n- Derivation of Sigmoid activation function:\\n  ```\\n  g(z)  = 1 / (1 + np.exp(-z))\\n  g'(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))\\n  g'(z) = g(z) * (1 - g(z))\\n  ```\\n- Derivation of Tanh activation function:\\n  ```\\n  g(z)  = (e^z - e^-z) / (e^z + e^-z)\\n  g'(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2\\n  ```\\n- Derivation of RELU activation function:\\n  ```\\n  g(z)  = np.maximum(0,z)\\n  g'(z) = { 0  if z < 0\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"g'(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2\\n  ```\\n- Derivation of RELU activation function:\\n  ```\\n  g(z)  = np.maximum(0,z)\\n  g'(z) = { 0  if z < 0\\n            1  if z >= 0  }\\n  ```\\n- Derivation of leaky RELU activation function:\\n  ```\\n  g(z)  = np.maximum(0.01 * z, z)\\n  g'(z) = { 0.01  if z < 0\\n            1     if z >= 0   }\\n  ```\\n### Gradient descent for Neural Networks\\n- In this section we will have the full back propagation of the neural network (Just the equations with no explanations).\\n- Gradient descent algorithm:\\n  - NN parameters:\\n    - `n[0] = Nx`\\n    - `n[1] = NoOfHiddenNeurons`\\n    - `n[2] = NoOfOutputNeurons = 1`\\n    - `W1` shape is `(n[1],n[0])`\\n    - `b1` shape is `(n[1],1)`\\n    - `W2` shape is `(n[2],n[1])`\\n    - `b2` shape is `(n[2],1)`\\n  - Cost function `I =  I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))`\\n  - Then Gradient descent:\\n    ```\\n    Repeat:\\n    \\t\\tCompute predictions (y'[i], i = 0,...m)\\n    \\t\\tGet derivatives: dW1, db1, dW2, db2\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- Then Gradient descent:\\n    ```\\n    Repeat:\\n    \\t\\tCompute predictions (y'[i], i = 0,...m)\\n    \\t\\tGet derivatives: dW1, db1, dW2, db2\\n    \\t\\tUpdate: W1 = W1 - LearningRate * dW1\\n    \\t\\t\\t\\tb1 = b1 - LearningRate * db1\\n    \\t\\t\\t\\tW2 = W2 - LearningRate * dW2\\n    \\t\\t\\t\\tb2 = b2 - LearningRate * db2\\n    ```\\n- Forward propagation:\\n  ```\\n  Z1 = W1A0 + b1    # A0 is X\\n  A1 = g1(Z1)\\n  Z2 = W2A1 + b2\\n  A2 = Sigmoid(Z2)      # Sigmoid because the output is between 0 and 1\\n  ```\\n- Backpropagation (derivations):   \\n  ```\\n  dZ2 = A2 - Y      # derivative of cost function we used * derivative of the sigmoid function\\n  dW2 = (dZ2 * A1.T) / m\\n  db2 = Sum(dZ2) / m\\n  dZ1 = (W2.T * dZ2) * g'1(Z1)  # element wise product (*)\\n  dW1 = (dZ1 * A0.T) / m   # A0 = X\\n  db1 = Sum(dZ1) / m\\n  # Hint there are transposes with multiplication because to keep dimensions correct\\n  ```\\n- How we derived the 6 equations of the backpropagation:   \\n  ![](Images/06.png)\\n### Random Initialization\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"```\\n- How we derived the 6 equations of the backpropagation:   \\n  ![](Images/06.png)\\n### Random Initialization\\n- In logistic regression it wasn't important to initialize the weights randomly, while in NN we have to initialize them randomly.\\n- If we initialize all the weights with zeros in NN it won't work (initializing bias with zero is OK):\\n  - all hidden units will be completely identical (symmetric) - compute exactly the same function\\n  - on each gradient descent iteration all the hidden units will always update the same\\n- To solve this we initialize the W's with a small random numbers:\\n  ```\\n  W1 = np.random.randn((2,2)) * 0.01    # 0.01 to make it small enough\\n  b1 = np.zeros((2,1))                  # its ok to have b as zero, it won't get us to the symmetry breaking problem\\n  ```\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"b1 = np.zeros((2,1))                  # its ok to have b as zero, it won't get us to the symmetry breaking problem\\n  ```\\n- We need small values because in sigmoid (or tanh), for example, if the weight is too large you are more likely to end up even at the very start of training with very large values of Z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue.\\n- Constant 0.01 is alright for 1 hidden layer networks, but if the NN is deep this number can be changed but it will always be a small number.\\n## Deep Neural Networks\\n> Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.\\n### Deep L-layer neural network\\n- Shallow NN is a NN with one or two layers.\\n- Deep NN is a NN with three or more layers.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"### Deep L-layer neural network\\n- Shallow NN is a NN with one or two layers.\\n- Deep NN is a NN with three or more layers.\\n- We will use the notation `L` to denote the number of layers in a NN.\\n- `n[l]` is the number of neurons in a specific layer `l`.\\n- `n[0]` denotes the number of neurons input layer. `n[L]` denotes the number of neurons in output layer.\\n- `g[l]` is the activation function.\\n- `a[l] = g[l](z[l])`\\n- `w[l]` weights is used for `z[l]`\\n- `x = a[0]`, `a[l] = y'`\\n- These were the notation we will use for deep neural network.\\n- So we have:\\n  - A vector `n` of shape `(1, NoOfLayers+1)`\\n  - A vector `g` of shape `(1, NoOfLayers)`\\n  - A list of different shapes `w` based on the number of neurons on the previous and the current layer.\\n  - A list of different shapes `b` based on the number of neurons on the current layer.\\n### Forward Propagation in a Deep Network\\n- Forward propagation general rule for one input:\\n  ```\\n  z[l] = W[l]a[l-1] + b[l]\\n  a[l] = g[l](a[l])\\n  ```\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"### Forward Propagation in a Deep Network\\n- Forward propagation general rule for one input:\\n  ```\\n  z[l] = W[l]a[l-1] + b[l]\\n  a[l] = g[l](a[l])\\n  ```\\n- Forward propagation general rule for `m` inputs:\\n  ```\\n  Z[l] = W[l]A[l-1] + B[l]\\n  A[l] = g[l](A[l])\\n  ```\\n- We can't compute the whole layers forward propagation without a for loop so its OK to have a for loop here.\\n- The dimensions of the matrices are so important you need to figure it out.\\n### Getting your matrix dimensions right\\n- The best way to debug your matrices dimensions is by a pencil and paper.\\n- Dimension of `W` is `(n[l],n[l-1])` . Can be thought by right to left.\\n- Dimension of `b` is `(n[l],1)`\\n- `dw` has the same shape as `W`, while `db` is the same shape as `b`\\n- Dimension of `Z[l],` `A[l]`, `dZ[l]`, and `dA[l]`  is `(n[l],m)`\\n### Why deep representations?\\n- Why deep NN works well, we will discuss this question in this section.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='### Why deep representations?\\n- Why deep NN works well, we will discuss this question in this section.\\n- Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relation with the previous layer. E.g.:\\n  - 1) Face recognition application:\\n      - Image ==> Edges ==> Face parts ==> Faces ==> desired face\\n  - 2) Audio recognition application:\\n      - Audio ==> Low level sound features like (sss,bb) ==> Phonemes ==> Words ==> Sentences\\n- Neural Researchers think that deep neural networks \"think\" like brains (simple ==> complex)\\n- Circuit theory and deep learning:\\n  - ![](Images/07.png)\\n- When starting on an application don\\'t start directly by dozens of hidden layers. Try the simplest solutions (e.g. Logistic Regression), then try the shallow neural network and so on.\\n### Building blocks of deep neural networks\\n- Forward and back propagation for a layer l:\\n  - ![Untitled](Images/10.png)\\n- Deep NN blocks:\\n  - ![](Images/08.png)'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- Forward and back propagation for a layer l:\\n  - ![Untitled](Images/10.png)\\n- Deep NN blocks:\\n  - ![](Images/08.png)\\n### Forward and Backward Propagation\\n- Pseudo code for forward propagation for layer l:\\n  ```\\n  Input  A[l-1]\\n  Z[l] = W[l]A[l-1] + b[l]\\n  A[l] = g[l](Z[l])\\n  Output A[l], cache(Z[l])\\n  ```\\n- Pseudo  code for back propagation for layer l:\\n  ```\\n  Input da[l], Caches\\n  dZ[l] = dA[l] * g'[l](Z[l])\\n  dW[l] = (dZ[l]A[l-1].T) / m\\n  db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True\\n  dA[l-1] = w[l].T * dZ[l]            # The multiplication here are a dot product.\\n  Output dA[l-1], dW[l], db[l]\\n  ```\\n- If we have used our loss function then:\\n  ```\\n  dA[L] = (-(y/a) + ((1-y)/(1-a)))\\n  ```\\n### Parameters vs Hyperparameters\\n- Main parameters of the NN is `W` and `b`\\n- Hyper parameters (parameters that control the algorithm) are like:\\n  - Learning rate.\\n  - Number of iteration.\\n  - Number of hidden layers `L`.\\n  - Number of hidden units `n`.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Learning rate.\\n  - Number of iteration.\\n  - Number of hidden layers `L`.\\n  - Number of hidden units `n`.\\n  - Choice of activation functions.\\n- You have to try values yourself of hyper parameters.\\n- In the earlier days of DL and ML learning rate was often called a parameter, but it really is (and now everybody call it) a hyperparameter.\\n- On the next course we will see how to optimize hyperparameters.\\n### What does this have to do with the brain\\n- The analogy that \"It is like the brain\" has become really an oversimplified explanation.\\n- There is a very simplistic analogy between a single logistic unit and a single neuron in the brain.\\n- No human today understand how a human brain neuron works.\\n- No human today know exactly how many neurons on the brain.\\n- Deep learning in Andrew\\'s opinion is very good at learning very flexible, complex functions to learn X to Y mappings, to learn input-output mappings (supervised learning).'), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content=\"- The field of computer vision has taken a bit more inspiration from the human brains then other disciplines that also apply deep learning.\\n- NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN)\\n## Extra: Ian Goodfellow interview\\n- Ian is one of the world's most visible deep learning researchers.\\n- Ian is mainly working with generative models. He is the creator of GANs.\\n- We need to stabilize GANs. Stabilized GANs can become the best generative models.\\n- Ian wrote the first textbook on the modern version of deep learning with Yoshua Bengio and Aaron Courville.\\n- Ian worked with [OpenAI.com](https://openai.com/) and Google on ML and NN applications.\\n- Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will find you.\"), Document(metadata={'source': 'docs\\\\Readme (2).md'}, page_content='- Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will find you.\\n- Ian thinks that we need to start anticipating security problems with ML now and make sure that these algorithms are secure from the start instead of trying to patch it in retroactively years later.\\n<br><br>\\n<br><br>\\nThese Notes were made by [Mahmoud Badry](mailto:mma18@fayoum.edu.eg) @2017'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\\nThis is the second course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.\\n## Table of contents\\n* [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](#improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization)\\n   * [Table of contents](#table-of-contents)\\n   * [Course summary](#course-summary)\\n   * [Practical aspects of Deep Learning](#practical-aspects-of-deep-learning)\\n      * [Train / Dev / Test sets](#train--dev--test-sets)\\n      * [Bias / Variance](#bias--variance)\\n      * [Basic Recipe for Machine Learning](#basic-recipe-for-machine-learning)\\n      * [Regularization](#regularization)\\n      * [Why regularization reduces overfitting?](#why-regularization-reduces-overfitting)'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='* [Regularization](#regularization)\\n      * [Why regularization reduces overfitting?](#why-regularization-reduces-overfitting)\\n      * [Dropout Regularization](#dropout-regularization)\\n      * [Understanding Dropout](#understanding-dropout)\\n      * [Other regularization methods](#other-regularization-methods)\\n      * [Normalizing inputs](#normalizing-inputs)\\n      * [Vanishing / Exploding gradients](#vanishing--exploding-gradients)\\n      * [Weight Initialization for Deep Networks](#weight-initialization-for-deep-networks)\\n      * [Numerical approximation of gradients](#numerical-approximation-of-gradients)\\n      * [Gradient checking implementation notes](#gradient-checking-implementation-notes)\\n      * [Initialization summary](#initialization-summary)\\n      * [Regularization summary](#regularization-summary)\\n   * [Optimization algorithms](#optimization-algorithms)\\n      * [Mini-batch gradient descent](#mini-batch-gradient-descent)'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='* [Optimization algorithms](#optimization-algorithms)\\n      * [Mini-batch gradient descent](#mini-batch-gradient-descent)\\n      * [Understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)\\n      * [Exponentially weighted averages](#exponentially-weighted-averages)\\n      * [Understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)\\n      * [Bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)\\n      * [Gradient descent with momentum](#gradient-descent-with-momentum)\\n      * [RMSprop](#rmsprop)\\n      * [Adam optimization algorithm](#adam-optimization-algorithm)\\n      * [Learning rate decay](#learning-rate-decay)\\n      * [The problem of local optima](#the-problem-of-local-optima)\\n   * [Hyperparameter tuning, Batch Normalization and Programming Frameworks](#hyperparameter-tuning-batch-normalization-and-programming-frameworks)\\n      * [Tuning process](#tuning-process)'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='* [Tuning process](#tuning-process)\\n      * [Using an appropriate scale to pick hyperparameters](#using-an-appropriate-scale-to-pick-hyperparameters)\\n      * [Hyperparameters tuning in practice: Pandas vs. Caviar](#hyperparameters-tuning-in-practice-pandas-vs-caviar)\\n      * [Normalizing activations in a network](#normalizing-activations-in-a-network)\\n      * [Fitting Batch Normalization into a neural network](#fitting-batch-normalization-into-a-neural-network)\\n      * [Why does Batch normalization work?](#why-does-batch-normalization-work)\\n      * [Batch normalization at test time](#batch-normalization-at-test-time)\\n      * [Softmax Regression](#softmax-regression)\\n      * [Training a Softmax classifier](#training-a-softmax-classifier)\\n      * [Deep learning frameworks](#deep-learning-frameworks)\\n      * [TensorFlow](#tensorflow)\\n   * [Extra Notes](#extra-notes)\\n## Course summary'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='* [Deep learning frameworks](#deep-learning-frameworks)\\n      * [TensorFlow](#tensorflow)\\n   * [Extra Notes](#extra-notes)\\n## Course summary\\nHere are the course summary as its given on the course [link](https://www.coursera.org/learn/deep-neural-network):\\n> This course will teach you the \"magic\" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. \\n>\\n> After 3 weeks, you will: \\n> - Understand industry best-practices for building deep learning applications. \\n> - Be able to effectively use the common neural network \"tricks\", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, \\n> - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='> - Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance\\n> - Be able to implement a neural network in TensorFlow. \\n>\\n> This is the second course of the Deep Learning Specialization.\\n## Practical aspects of Deep Learning\\n### Train / Dev / Test sets\\n- Its impossible to get all your hyperparameters right on a new application from the first time.\\n- So the idea is you go through the loop: `Idea ==> Code ==> Experiment`.\\n- You have to go through the loop many times to figure out your hyperparameters.\\n- Your data will be split into three parts:\\n  - Training set.       (Has to be the largest set)\\n  - Hold-out cross validation set / Development or \"dev\" set.\\n  - Testing set.\\n- You will try to build a model upon training set then try to optimize hyperparameters on dev set as much as possible. Then after your model is ready you try and evaluate the testing set.\\n- so the trend on the ratio of splitting the models:'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- so the trend on the ratio of splitting the models:\\n  - If size of the  dataset is 100 to 1000000  ==> 60/20/20\\n  - If size of the  dataset is 1000000  to INF  ==> 98/1/1 or  99.5/0.25/0.25\\n- The trend now gives the training data the biggest sets.\\n- Make sure the dev and test set are coming from the same distribution.\\n  - For example if cat training/dev pictures are from the web but the test pictures are from users cell phone they will mismatch. It is better to make sure that dev and test set are from the same distribution.\\n- The dev set rule is to try them on some of the good models you've created.\\n- Its OK to only have a dev set without a testing set. But a lot of people in this case call the dev set as the test set. A better terminology is to call it a dev set as its used in the development.\\n### Bias / Variance\\n- Bias / Variance techniques are Easy to learn, but difficult to master.\\n- So here the explanation of Bias / Variance:\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='### Bias / Variance\\n- Bias / Variance techniques are Easy to learn, but difficult to master.\\n- So here the explanation of Bias / Variance:\\n  - If your model is underfitting (logistic regression of non linear data) it has a \"high bias\"\\n  - If your model is overfitting then it has a \"high variance\"\\n  - Your model will be alright if you balance the Bias / Variance\\n  - For more:\\n    - ![](Images/01-_Bias_-_Variance.png)\\n- Another idea to get the bias /  variance if you don\\'t have a 2D plotting mechanism:\\n  - High variance (overfitting) for example:\\n    - Training error: 1%\\n    - Dev error: 11%\\n  - high Bias (underfitting) for example:\\n    - Training error: 15%\\n    - Dev error: 14%\\n  - high Bias (underfitting) && High variance (overfitting) for example:\\n    - Training error: 15%\\n    - Test error: 30%\\n  - Best:\\n    - Training error: 0.5%\\n    - Test error: 1%\\n  - These Assumptions came from that human has 0% error. If the problem isn\\'t like that you\\'ll need to use human error as baseline.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- These Assumptions came from that human has 0% error. If the problem isn\\'t like that you\\'ll need to use human error as baseline.\\n### Basic Recipe for Machine Learning\\n- If your algorithm has a high bias:\\n  - Try to make your NN bigger (size of hidden units, number of layers)\\n  - Try a different model that is suitable for your data.\\n  - Try to run it longer.\\n  - Different (advanced) optimization algorithms.\\n- If your algorithm has a high variance:\\n  - More data.\\n  - Try regularization.\\n  - Try a different model that is suitable for your data.\\n- You should try the previous two points until you have a low bias and low variance.\\n- In the older days before deep learning, there was a \"Bias/variance tradeoff\". But because now you have more options/tools for solving the bias and variance problem its really helpful to use deep learning.\\n- Training a bigger neural network never hurts.\\n### Regularization\\n- Adding regularization to NN will help it reduce variance (overfitting)\\n- L1 matrix norm:'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"### Regularization\\n- Adding regularization to NN will help it reduce variance (overfitting)\\n- L1 matrix norm:\\n  - `||W|| = Sum(|w[i,j]|)  # sum of absolute values of all w`\\n- L2 matrix norm because of arcane technical math reasons is called Frobenius norm:\\n  - `||W||^2 = Sum(|w[i,j]|^2)\\t# sum of all w squared`\\n  - Also can be calculated as `||W||^2 = W.T * W if W is a vector`\\n- Regularization for logistic regression:\\n  - The normal cost function that we want to minimize is: `J(w,b) = (1/m) * Sum(L(y(i),y'(i)))`\\n  - The L2 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|^2)`\\n  - The L1 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|)`\\n  - The L1 regularization version makes a lot of w values become zeros, which makes the model size smaller.\\n  - L2 regularization is being used much more often.\\n  - `lambda` here is the regularization parameter (hyperparameter)\\n- Regularization for NN:\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- L2 regularization is being used much more often.\\n  - `lambda` here is the regularization parameter (hyperparameter)\\n- Regularization for NN:\\n  - The normal cost function that we want to minimize is:   \\n    `J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i)))`\\n  - The L2 regularization version:   \\n    `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2)`\\n  - We stack the matrix as one vector `(mn,1)` and then we apply `sqrt(w1^2 + w2^2.....)`\\n  - To do back propagation (old way):   \\n    `dw[l] = (from back propagation)`\\n  - The new way:   \\n    `dw[l] = (from back propagation) + lambda/m * w[l]`\\n  - So plugging it in weight update step:\\n    - ```\\n      w[l] = w[l] - learning_rate * dw[l]\\n           = w[l] - learning_rate * ((from back propagation) + lambda/m * w[l])\\n           = w[l] - (learning_rate*lambda/m) * w[l] - learning_rate * (from back propagation) \\n           = (1 - (learning_rate*lambda)/m) * w[l] - learning_rate * (from back propagation)\\n      ```\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"= (1 - (learning_rate*lambda)/m) * w[l] - learning_rate * (from back propagation)\\n      ```\\n  - In practice this penalizes large weights and effectively limits the freedom in your model.\\n  - The new term `(1 - (learning_rate*lambda)/m) * w[l]`  causes the **weight to decay** in proportion to its size.\\n### Why regularization reduces overfitting?\\nHere are some intuitions:\\n  - Intuition 1:\\n     - If `lambda` is too large - a lot of w's will be close to zeros which will make the NN simpler (you can think of it as it would behave closer to logistic regression).\\n     - If `lambda` is good enough it will just reduce some weights that makes the neural network overfit.\\n  - Intuition 2 (with _tanh_ activation function):\\n     - If `lambda` is too large, w's will be small (close to zero) - will use the linear part of the _tanh_ activation function, so we will go from non linear activation to _roughly_ linear which would make the NN a _roughly_ linear classifier.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- If `lambda` good enough it will just make some of _tanh_ activations _roughly_ linear which will prevent overfitting.\\n     \\n_**Implementation tip**_: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J as a function of the number of iterations of gradient descent and you want to see that the cost function J decreases **monotonically** after every elevation of gradient descent with regularization. If you plot the old definition of J (no regularization) then you might not see it decrease monotonically.\\n### Dropout Regularization\\n- In most cases Andrew Ng tells that he uses the L2 regularization.\\n- The dropout regularization eliminates some neurons/weights on each iteration based on a probability.\\n- A most common technique to implement dropout is called \"Inverted dropout\".\\n- Code for Inverted dropout:\\n  ```python\\n  keep_prob = 0.8   # 0 <= keep_prob <= 1\\n  l = 3  # this code is only for layer 3'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- Code for Inverted dropout:\\n  ```python\\n  keep_prob = 0.8   # 0 <= keep_prob <= 1\\n  l = 3  # this code is only for layer 3\\n  # the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped\\n  d3 = np.random.rand(a[l].shape[0], a[l].shape[1]) < keep_prob\\n  a3 = np.multiply(a3,d3)   # keep only the values in d3\\n  # increase a3 to not reduce the expected value of output\\n  # (ensures that the expected value of a3 remains the same) - to solve the scaling problem\\n  a3 = a3 / keep_prob       \\n  ```\\n- Vector d[l] is used for forward and back propagation and is the same for them, but it is different for each iteration (pass) or training example.\\n- At test time we don't use dropout. If you implement dropout at test time - it would add noise to predictions.\\n### Understanding Dropout\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- At test time we don't use dropout. If you implement dropout at test time - it would add noise to predictions.\\n### Understanding Dropout\\n- In the previous video, the intuition was that dropout randomly knocks out units in your network. So it's as if on every iteration you're working with a smaller NN, and so using a smaller NN seems like it should have a regularizing effect.\\n- Another intuition: can't rely on any one feature, so have to spread out weights.\\n- It's possible to show that dropout has a similar effect to L2 regularization.\\n- Dropout can have different `keep_prob` per layer.\\n- The input layer dropout has to be near 1 (or 1 - no dropout) because you don't want to eliminate a lot of features.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- The input layer dropout has to be near 1 (or 1 - no dropout) because you don't want to eliminate a lot of features.\\n- If you're more worried about some layers overfitting than others, you can set a lower `keep_prob` for some layers than others. The downside is, this gives you even more hyperparameters to search for using cross-validation. One other alternative might be to have some layers where you apply dropout and some layers where you don't apply dropout and then just have one hyperparameter, which is a `keep_prob` for the layers for which you do apply dropouts.\\n- A lot of researchers are using dropout with Computer Vision (CV) because they have a very big input size and almost never have enough data, so overfitting is the usual problem. And dropout is a regularization technique to prevent overfitting.\\n- A downside of dropout is that the cost function J is not well defined and it will be hard to debug (plot J by iteration).\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- A downside of dropout is that the cost function J is not well defined and it will be hard to debug (plot J by iteration).\\n  - To solve that you'll need to turn off dropout, set all the `keep_prob`s to 1, and then run the code and check that it monotonically decreases J and then turn on the dropouts again.\\n### Other regularization methods\\n- **Data augmentation**:\\n  - For example in a computer vision data:\\n    - You can flip all your pictures horizontally this will give you m more data instances.\\n    - You could also apply a random position and rotation to an image to get more data.\\n  - For example in OCR, you can impose random rotations and distortions to digits/letters.\\n  - New data obtained using this technique isn't as good as the real independent data, but still can be used as a regularization technique.\\n- **Early stopping**:\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- **Early stopping**:\\n  - In this technique we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing.\\n  - We will pick the point at which the training set error and dev set error are best (lowest training cost with lowest dev cost).\\n  - We will take these parameters as the best parameters.\\n    - ![](Images/02-_Early_stopping.png)\\n  - Andrew prefers to use L2 regularization instead of early stopping because this technique simultaneously tries to minimize the cost function and not to overfit which contradicts the orthogonalization approach (will be discussed further).\\n  - But its advantage is that you don't need to search a hyperparameter like in other regularization approaches (like `lambda` in L2 regularization).\\n- **Model Ensembles**:\\n  - Algorithm:\\n    - Train multiple independent models.\\n    - At test time average their results.\\n  - It can get you extra 2% performance.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- Algorithm:\\n    - Train multiple independent models.\\n    - At test time average their results.\\n  - It can get you extra 2% performance.\\n  - It reduces the generalization error.\\n  - You can use some snapshots of your NN at the training ensembles them and take the results.\\n### Normalizing inputs\\n- If you normalize your inputs this will speed up the training process a lot.\\n- Normalization are going on these steps:\\n  1. Get the mean of the training set: `mean = (1/m) * sum(x(i))`\\n  2. Subtract the mean from each input: `X = X - mean`\\n     - This makes your inputs centered around 0.\\n  3. Get the variance of the training set: `variance = (1/m) * sum(x(i)^2)`\\n  4. Normalize the variance. `X /= variance`\\n- These steps should be applied to training, dev, and testing sets (but using mean and variance of the train set).\\n- Why normalize?\\n  - If we don't normalize the inputs our cost function will be deep and its shape will be inconsistent (elongated) then optimizing it will take a long time.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- But if we normalize it the opposite will occur. The shape of the cost function will be consistent (look more symmetric like circle in 2D example) and we can use a larger learning rate alpha - the optimization will be faster.\\n### Vanishing / Exploding gradients\\n- The Vanishing / Exploding gradients occurs when your derivatives become very small or very big.\\n- To understand the problem, suppose that we have a deep neural network with number of layers L, and all the activation functions are **linear** and each `b = 0`\\n  - Then:   \\n    ```\\n    Y' = W[L]W[L-1].....W[2]W[1]X\\n    ```\\n  - Then, if we have 2 hidden units per layer and x1 = x2 = 1, we result in:\\n    ```\\n    if W[l] = [1.5   0] \\n              [0   1.5] (l != L because of different dimensions in the output layer)\\n    Y' = W[L] [1.5  0]^(L-1) X = 1.5^L \\t# which will be very large\\n              [0  1.5]\\n    ```\\n    ```\\n    if W[l] = [0.5  0]\\n              [0  0.5]\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"[0  1.5]\\n    ```\\n    ```\\n    if W[l] = [0.5  0]\\n              [0  0.5]\\n    Y' = W[L] [0.5  0]^(L-1) X = 0.5^L \\t# which will be very small\\n              [0  0.5]\\n    ```\\n- The last example explains that the activations (and similarly derivatives) will be decreased/increased exponentially as a function of number of layers.\\n- So If W > I (Identity matrix) the activation and gradients will explode.\\n- And If W < I (Identity matrix) the activation and gradients will vanish.\\n- Recently Microsoft trained 152 layers (ResNet)! which is a really big number. With such a deep neural network, if your activations or gradients increase or decrease exponentially as a function of L, then these values could get really big or really small. And this makes training difficult, especially if your gradients are exponentially smaller than L, then gradient descent will take tiny little steps. It will take a long time for gradient descent to learn anything.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- There is a partial solution that doesn't completely solve this problem but it helps a lot - careful choice of how you initialize the weights (next video).\\n### Weight Initialization for Deep Networks\\n- A partial solution to the Vanishing / Exploding gradients in NN is better or more careful choice of the random initialization of weights\\n- In a single neuron (Perceptron model): `Z = w1x1 + w2x2 + ... + wnxn`\\n  - So if `n_x` is large we want `W`'s to be smaller to not explode the cost.\\n- So it turns out that we need the variance which equals `1/n_x` to be the range of `W`'s\\n- So lets say when we initialize `W`'s like this (better to use with `tanh` activation):   \\n  ```\\n  np.random.rand(shape) * np.sqrt(1/n[l-1])\\n  ```\\n  or variation of this (Bengio et al.):   \\n  ```\\n  np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))\\n  ```\\n- Setting initialization part inside sqrt to `2/n[l-1]` for `ReLU` is better:   \\n  ```\\n  np.random.rand(shape) * np.sqrt(2/n[l-1])\\n  ```\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='```\\n- Setting initialization part inside sqrt to `2/n[l-1]` for `ReLU` is better:   \\n  ```\\n  np.random.rand(shape) * np.sqrt(2/n[l-1])\\n  ```\\n- Number 1 or 2 in the neumerator can also be a hyperparameter to tune (but not the first to start with)\\n- This is one of the best way of partially solution to Vanishing / Exploding gradients (ReLU + Weight Initialization with variance) which will help gradients not to vanish/explode too quickly\\n- The initialization in this video is called \"He Initialization / Xavier Initialization\" and has been published in 2015 paper.\\n### Numerical approximation of gradients\\n- There is an technique called gradient checking which tells you if your implementation of backpropagation is correct.\\n- There\\'s a numerical way to calculate the derivative:   \\n  ![](Images/03-_Numerical_approximation_of_gradients.png)'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- There's a numerical way to calculate the derivative:   \\n  ![](Images/03-_Numerical_approximation_of_gradients.png)\\n- Gradient checking approximates the gradients and is very helpful for finding the errors in your backpropagation implementation but it's slower than gradient descent (so use only for debugging).\\n- Implementation of this is very simple.\\n- Gradient checking:\\n  - First take `W[1],b[1],...,W[L],b[L]` and reshape into one big vector (`theta`)\\n  - The cost function will be `J(theta)`\\n  - Then take `dW[1],db[1],...,dW[L],db[L]` into one big vector (`d_theta`)\\n  - **Algorithm**:   \\n    ```\\n    eps = 10^-7   # small number\\n    for i in len(theta):\\n      d_theta_approx[i] = (J(theta1,...,theta[i] + eps) -  J(theta1,...,theta[i] - eps)) / 2*eps\\n    ```\\n  - Finally we evaluate this formula `(||d_theta_approx - d_theta||) / (||d_theta_approx||+||d_theta||)` (`||` - Euclidean vector norm) and check (with eps = 10^-7):\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- if it is < 10^-7  - great, very likely the backpropagation implementation is correct\\n    - if around 10^-5   - can be OK, but need to inspect if there are no particularly big values in `d_theta_approx - d_theta` vector\\n    - if it is >= 10^-3 - bad, probably there is a bug in backpropagation implementation\\n### Gradient checking implementation notes\\n- Don't use the gradient checking algorithm at training time because it's very slow.\\n- Use gradient checking only for debugging.\\n- If algorithm fails grad check, look at components to try to identify the bug.\\n- Don't forget to add `lamda/(2m) * sum(W[l])` to `J` if you are using L1 or L2 regularization.\\n- Gradient checking doesn't work with dropout because J is not consistent. \\n  - You can first turn off dropout (set `keep_prob = 1.0`), run gradient checking and then turn on dropout again.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- You can first turn off dropout (set `keep_prob = 1.0`), run gradient checking and then turn on dropout again.\\n- Run gradient checking at random initialization and train the network for a while maybe there's a bug which can be seen when w's and b's become larger (further from 0) and can't be seen on the first iteration (when w's and b's are very small).\\n### Initialization summary\\n- The weights W<sup>[l]</sup> should be initialized randomly to break symmetry\\n- It is however okay to initialize the biases b<sup>[l]</sup> to zeros. Symmetry is still broken so long as W<sup>[l]</sup> is initialized randomly\\n- Different initializations lead to different results\\n- Random initialization is used to break symmetry and make sure different hidden units can learn different things\\n- Don't intialize to values that are too large\\n- He initialization works well for networks with ReLU activations. \\n### Regularization summary\\n#### 1. L2 Regularization   \\n**Observations**:\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- He initialization works well for networks with ReLU activations. \\n### Regularization summary\\n#### 1. L2 Regularization   \\n**Observations**:   \\n  - The value of λ is a hyperparameter that you can tune using a dev set.\\n  - L2 regularization makes your decision boundary smoother. If λ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\\n**What is L2-regularization actually doing?**:   \\n  - L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\\n**What you should remember:**   \\nImplications of L2-regularization on:\\n  - cost computation:\\n    - A regularization term is added to the cost\\n  - backpropagation function:'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='Implications of L2-regularization on:\\n  - cost computation:\\n    - A regularization term is added to the cost\\n  - backpropagation function:\\n    - There are extra terms in the gradients with respect to weight matrices\\n  - weights:\\n    - weights end up smaller (\"weight decay\") - are pushed to smaller values.\\n    \\n#### 2. Dropout   \\n**What you should remember about dropout:**   \\n- Dropout is a regularization technique.\\n- You only use dropout during training. Don\\'t use dropout (randomly eliminate nodes) during test time.\\n- Apply dropout both during forward and backward propagation.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- Apply dropout both during forward and backward propagation.\\n- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if `keep_prob` is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.\\n## Optimization algorithms\\n### Mini-batch gradient descent\\n- Training NN with a large data is slow. So to find an optimization algorithm that runs faster is a good idea.\\n- Suppose we have `m = 50 million`. To train this data it will take a huge processing time for one step.\\n  - because 50 million won't fit in the memory at once we need other processing to make such a thing.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- because 50 million won't fit in the memory at once we need other processing to make such a thing.\\n- It turns out you can make a faster algorithm to make gradient descent process some of your items even before you finish the 50 million items.\\n- Suppose we have split m to **mini batches** of size 1000.\\n  - `X{1} = 0    ...  1000`\\n  - `X{2} = 1001 ...  2000`\\n  - `...`\\n  - `X{bs} = ...`\\n- We similarly split `X` & `Y`.\\n- So the definition of mini batches ==> `t: X{t}, Y{t}`\\n- In **Batch gradient descent** we run the gradient descent on the whole dataset.\\n- While in **Mini-Batch gradient descent** we run the gradient descent on the mini datasets.\\n- Mini-Batch algorithm pseudo code:\\n  ```\\n  for t = 1:No_of_batches                         # this is called an epoch\\n  \\tAL, caches = forward_prop(X{t}, Y{t})\\n  \\tcost = compute_cost(AL, Y{t})\\n  \\tgrads = backward_prop(AL, caches)\\n  \\tupdate_parameters(grads)\\n  ```\\n- The code inside an epoch should be vectorized.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"grads = backward_prop(AL, caches)\\n  \\tupdate_parameters(grads)\\n  ```\\n- The code inside an epoch should be vectorized.\\n- Mini-batch gradient descent works much faster in the large datasets.\\n### Understanding mini-batch gradient descent\\n- In mini-batch algorithm, the cost won't go down with each step as it does in batch algorithm. It could contain some ups and downs but generally it has to go down (unlike the batch gradient descent where cost function descreases on each iteration).\\n  ![](Images/04-_batch_vs_mini_batch_cost.png)\\n- Mini-batch size:\\n  - (`mini batch size = m`)  ==>    Batch gradient descent\\n  - (`mini batch size = 1`)  ==>    Stochastic gradient descent (SGD)\\n  - (`mini batch size = between 1 and m`) ==>    Mini-batch gradient descent\\n- Batch gradient descent:\\n  - too long per iteration (epoch)\\n- Stochastic gradient descent:\\n  - too noisy regarding cost minimization (can be reduced by using smaller learning rate)\\n  - won't ever converge (reach the minimum cost)\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- too noisy regarding cost minimization (can be reduced by using smaller learning rate)\\n  - won't ever converge (reach the minimum cost)\\n  - lose speedup from vectorization\\n- Mini-batch gradient descent:\\n  1. faster learning:\\n      - you have the vectorization advantage\\n      - make progress without waiting to process the entire training set\\n  2. doesn't always exactly converge (oscelates in a very small region, but you can reduce learning rate)\\n- Guidelines for choosing mini-batch size:\\n  1. If small training set (< 2000 examples) - use batch gradient descent.\\n  2. It has to be a power of 2 (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2):\\n    `64, 128, 256, 512, 1024, ...`\\n  3. Make sure that mini-batch fits in CPU/GPU memory.\\n- Mini-batch size is a `hyperparameter`.\\n### Exponentially weighted averages\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='3. Make sure that mini-batch fits in CPU/GPU memory.\\n- Mini-batch size is a `hyperparameter`.\\n### Exponentially weighted averages\\n- There are optimization algorithms that are better than **gradient descent**, but you should first learn about Exponentially weighted averages.\\n- If we have data like the temperature of day through the year it could be like this:\\n  ```\\n  t(1) = 40\\n  t(2) = 49\\n  t(3) = 45\\n  ...\\n  t(180) = 60\\n  ...\\n  ```\\n- This data is small in winter and big in summer. If we plot this data we will find it some noisy.\\n- Now lets compute the Exponentially weighted averages:\\n  ```\\n  V0 = 0\\n  V1 = 0.9 * V0 + 0.1 * t(1) = 4\\t\\t# 0.9 and 0.1 are hyperparameters\\n  V2 = 0.9 * V1 + 0.1 * t(2) = 8.5\\n  V3 = 0.9 * V2 + 0.1 * t(3) = 12.15\\n  ...\\n  ```\\n- General equation\\n  ```\\n  V(t) = beta * v(t-1) + (1-beta) * theta(t)\\n  ```\\n- If we plot this it will represent averages over `~ (1 / (1 - beta))` entries:\\n    - `beta = 0.9` will average last 10 entries'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='```\\n- If we plot this it will represent averages over `~ (1 / (1 - beta))` entries:\\n    - `beta = 0.9` will average last 10 entries\\n    - `beta = 0.98` will average last 50 entries\\n    - `beta = 0.5` will average last 2 entries\\n- Best beta average for our case is between 0.9 and 0.98\\n- **Intuition**: The reason why exponentially weighted averages are useful for further optimizing gradient descent algorithm is that it can give different weights to recent data points (`theta`) based on value of `beta`. If `beta` is high (around 0.9), it smoothens out the averages of skewed data points (oscillations w.r.t. Gradient descent terminology). So this reduces oscillations in gradient descent and hence makes faster and smoother path towerds minima.\\n- Another imagery example:   \\n    ![](Images/Nasdaq1_small.png)   \\n    _(taken from [investopedia.com](https://www.investopedia.com/))_\\n### Understanding exponentially weighted averages\\n- Intuitions:'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='_(taken from [investopedia.com](https://www.investopedia.com/))_\\n### Understanding exponentially weighted averages\\n- Intuitions:   \\n    ![](Images/05-_exponentially_weighted_averages_intuitions.png)\\n- We can implement this algorithm with more accurate results using a moving window. But the code is more efficient and faster using the exponentially weighted averages algorithm.\\n- Algorithm is very simple:\\n  ```\\n  v = 0\\n  Repeat\\n  {\\n  \\tGet theta(t)\\n  \\tv = beta * v + (1-beta) * theta(t)\\n  }\\n  ```\\n### Bias correction in exponentially weighted averages\\n- The bias correction helps make the exponentially weighted averages more accurate.\\n- Because `v(0) = 0`, the bias of the weighted averages is shifted and the accuracy suffers at the start.\\n- To solve the bias issue we have to use this equation:\\n  ```\\n  v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)\\n  ```\\n- As t becomes larger the `(1 - beta^t)` becomes close to `1`\\n### Gradient descent with momentum'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"```\\n- As t becomes larger the `(1 - beta^t)` becomes close to `1`\\n### Gradient descent with momentum\\n- The momentum algorithm almost always works faster than standard gradient descent.\\n- The simple idea is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.\\n- Pseudo code:\\n  ```\\n  vdW = 0, vdb = 0\\n  on iteration t:\\n  \\t# can be mini-batch or batch gradient descent\\n  \\tcompute dw, db on current mini-batch                \\n  \\t\\t\\t\\n  \\tvdW = beta * vdW + (1 - beta) * dW\\n  \\tvdb = beta * vdb + (1 - beta) * db\\n  \\tW = W - learning_rate * vdW\\n  \\tb = b - learning_rate * vdb\\n  ```\\n- Momentum helps the cost function to go to the minimum point in a more fast and consistent way.\\n- `beta` is another `hyperparameter`. `beta = 0.9` is very common and works very well in most cases.\\n- In practice people don't bother implementing **bias correction**.\\n### RMSprop\\n- Stands for **Root mean square prop**.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- In practice people don't bother implementing **bias correction**.\\n### RMSprop\\n- Stands for **Root mean square prop**.\\n- This algorithm speeds up the gradient descent.\\n- Pseudo code:\\n  ```\\n  sdW = 0, sdb = 0\\n  on iteration t:\\n  \\t# can be mini-batch or batch gradient descent\\n  \\tcompute dw, db on current mini-batch\\n  \\t\\n  \\tsdW = (beta * sdW) + (1 - beta) * dW^2  # squaring is element-wise\\n  \\tsdb = (beta * sdb) + (1 - beta) * db^2  # squaring is element-wise\\n  \\tW = W - learning_rate * dW / sqrt(sdW)\\n  \\tb = B - learning_rate * db / sqrt(sdb)\\n  ```\\n- RMSprop will make the cost function move slower on the vertical direction and faster on the horizontal direction in the following example:\\n    ![](Images/06-_RMSprop.png)\\n- Ensure that `sdW` is not zero by adding a small value `epsilon` (e.g. `epsilon = 10^-8`) to it:   \\n   `W = W - learning_rate * dW / (sqrt(sdW) + epsilon)`\\n- With RMSprop you can increase your learning rate.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='`W = W - learning_rate * dW / (sqrt(sdW) + epsilon)`\\n- With RMSprop you can increase your learning rate.\\n- Developed by Geoffrey Hinton and firstly introduced on [Coursera.org](https://www.coursera.org/) course.\\n### Adam optimization algorithm\\n- Stands for **Adaptive Moment Estimation**.\\n- Adam optimization and RMSprop are among the optimization algorithms that worked very well with a lot of NN architectures.\\n- Adam optimization simply puts RMSprop and momentum together!\\n- Pseudo code:\\n  ```\\n  vdW = 0, vdW = 0\\n  sdW = 0, sdb = 0\\n  on iteration t:\\n  \\t# can be mini-batch or batch gradient descent\\n  \\tcompute dw, db on current mini-batch                \\n  \\t\\t\\t\\n  \\tvdW = (beta1 * vdW) + (1 - beta1) * dW     # momentum\\n  \\tvdb = (beta1 * vdb) + (1 - beta1) * db     # momentum\\n  \\t\\t\\t\\n  \\tsdW = (beta2 * sdW) + (1 - beta2) * dW^2   # RMSprop\\n  \\tsdb = (beta2 * sdb) + (1 - beta2) * db^2   # RMSprop\\n  \\t\\t\\t\\n  \\tvdW = vdW / (1 - beta1^t)      # fixing bias\\n  \\tvdb = vdb / (1 - beta1^t)      # fixing bias'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"vdW = vdW / (1 - beta1^t)      # fixing bias\\n  \\tvdb = vdb / (1 - beta1^t)      # fixing bias\\n  \\t\\t\\t\\n  \\tsdW = sdW / (1 - beta2^t)      # fixing bias\\n  \\tsdb = sdb / (1 - beta2^t)      # fixing bias\\n  \\t\\t\\t\\t\\t\\n  \\tW = W - learning_rate * vdW / (sqrt(sdW) + epsilon)\\n  \\tb = B - learning_rate * vdb / (sqrt(sdb) + epsilon)\\n  ```\\n- Hyperparameters for Adam:\\n  - Learning rate: needed to be tuned.\\n  - `beta1`: parameter of the momentum - `0.9` is recommended by default.\\n  - `beta2`: parameter of the RMSprop - `0.999` is recommended by default.\\n  - `epsilon`: `10^-8` is recommended by default.\\n### Learning rate decay\\n- Slowly reduce learning rate.\\n- As mentioned before mini-batch gradient descent won't reach the optimum point (converge). But by making the learning rate decay with iterations it will be much closer to it because the steps (and possible oscillations) near the optimum are smaller.\\n- One technique equations is`learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate_0`\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- One technique equations is`learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate_0`  \\n  - `epoch_num` is over all data (not a single mini-batch).\\n- Other learning rate decay methods (continuous):\\n  - `learning_rate = (0.95 ^ epoch_num) * learning_rate_0`\\n  - `learning_rate = (k / sqrt(epoch_num)) * learning_rate_0`\\n- Some people perform learning rate decay discretely - repeatedly decrease after some number of epochs.\\n- Some people are making changes to the learning rate manually.\\n- `decay_rate` is another `hyperparameter`.\\n- For Andrew Ng, learning rate decay has less priority.\\n### The problem of local optima\\n- The normal local optima is not likely to appear in a deep neural network because data is usually high dimensional. For point to be a local optima it has to be a local optima for each of the dimensions which is highly unlikely.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- It's unlikely to get stuck in a bad local optima in high dimensions, it is much more likely to get to the saddle point rather to the local optima, which is not a problem.\\n- Plateaus can make learning slow:\\n  - Plateau is a region where the derivative is close to zero for a long time.\\n  - This is where algorithms like momentum, RMSprop or Adam can help.\\n## Hyperparameter tuning, Batch Normalization and Programming Frameworks\\n### Tuning process\\n- We need to tune our hyperparameters to get the best out of them.\\n- Hyperparameters importance are (as for Andrew Ng):\\n  1. Learning rate.\\n  2. Momentum beta.\\n  3. Mini-batch size.\\n  4. No. of hidden units.\\n  5. No. of layers.\\n  6. Learning rate decay.\\n  7. Regularization lambda.\\n  8. Activation functions.\\n  9. Adam `beta1`, `beta2` & `epsilon`.\\n- Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\\n- One of the ways to tune is to sample a grid with `N` hyperparameter settings and then try all settings combinations on your problem.\\n- Try random values: don\\'t use a grid.\\n- You can use `Coarse to fine sampling scheme`:\\n  - When you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.\\n- These methods can be automated.\\n### Using an appropriate scale to pick hyperparameters\\n- Let\\'s say you have a specific range for a hyperparameter from \"a\" to \"b\". It\\'s better to search for the right ones using the logarithmic scale rather then in linear scale:\\n  - Calculate: `a_log = log(a)  # e.g. a = 0.0001 then a_log = -4`\\n  - Calculate: `b_log = log(b)  # e.g. b = 1  then b_log = 0`\\n  - Then:\\n    ```\\n    r = (a_log - b_log) * np.random.rand() + b_log'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- Calculate: `b_log = log(b)  # e.g. b = 1  then b_log = 0`\\n  - Then:\\n    ```\\n    r = (a_log - b_log) * np.random.rand() + b_log\\n    # In the example the range would be from [-4, 0] because rand range [0,1)\\n    result = 10^r\\n    ```\\n    It uniformly samples values in log scale from [a,b].\\n- If we want to use the last method on exploring on the \"momentum beta\":\\n  - Beta best range is from 0.9 to 0.999.\\n  - You should search for `1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999)` and the use `a = 0.001` and `b = 0.1`. Then:\\n    ```\\n    a_log = -3\\n    b_log = -1\\n    r = (a_log - b_log) * np.random.rand() + b_log\\n    beta = 1 - 10^r   # because 1 - beta = 10^r\\n    ```\\n### Hyperparameters tuning in practice: Pandas vs. Caviar \\n- Intuitions about hyperparameter settings from one application area may or may not transfer to a different one.\\n- If you don\\'t have much computational resources you can use the \"babysitting model\":'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- If you don\\'t have much computational resources you can use the \"babysitting model\":\\n  - Day 0 you might initialize your parameter as random and then start training.\\n  - Then you watch your learning curve gradually decrease over the day.\\n  - And each day you nudge your parameters a little during training.\\n  - Called panda approach.\\n- If you have enough computational resources, you can run some models in parallel and at the end of the day(s) you check the results.\\n  - Called Caviar approach.\\n### Normalizing activations in a network\\n- In the rise of deep learning, one of the most important ideas has been an algorithm called **batch normalization**, created by two researchers, Sergey Ioffe and Christian Szegedy.\\n- Batch Normalization speeds up learning.\\n- Before we normalized input by subtracting the mean and dividing by variance. This helped a lot for the shape of the cost function and for reaching the minimum point faster.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- The question is: *for any hidden layer can we normalize `A[l]` to train `W[l+1]`, `b[l+1]` faster?* This is what batch normalization is about.\\n- There are some debates in the deep learning literature about whether you should normalize values before the activation function `Z[l]` or after applying the activation function `A[l]`. In practice, normalizing `Z[l]` is done much more often and that is what Andrew Ng presents.\\n- Algorithm:\\n  - Given `Z[l] = [z(1), ..., z(m)]`, i = 1 to m (for each input)\\n  - Compute `mean = 1/m * sum(z[i])`\\n  - Compute `variance = 1/m * sum((z[i] - mean)^2)`\\n  - Then `Z_norm[i] = (z[i] - mean) / np.sqrt(variance + epsilon)` (add `epsilon` for numerical stability if variance = 0)\\n    - Forcing the inputs to a distribution with zero mean and variance of 1.\\n  - Then `Z_tilde[i] = gamma * Z_norm[i] + beta`\\n    - To make inputs belong to other distribution (with other mean and variance).\\n    - gamma and beta are learnable parameters of the model.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- To make inputs belong to other distribution (with other mean and variance).\\n    - gamma and beta are learnable parameters of the model.\\n    - Making the NN learn the distribution of the outputs.\\n    - _Note:_ if `gamma = sqrt(variance + epsilon)` and `beta = mean` then `Z_tilde[i] = z[i]`\\n### Fitting Batch Normalization into a neural network\\n- Using batch norm in 3 hidden layers NN:\\n    ![](Images/bn.png)\\n- Our NN parameters will be:\\n  - `W[1]`, `b[1]`, ..., `W[L]`, `b[L]`, `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]`\\n  - `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]` are updated using any optimization algorithms (like GD, RMSprop, Adam)\\n- If you are using a deep learning framework, you won't have to implement batch norm yourself:\\n  - Ex. in Tensorflow you can add this line: `tf.nn.batch-normalization()`\\n- Batch normalization is usually applied with mini-batches.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- Ex. in Tensorflow you can add this line: `tf.nn.batch-normalization()`\\n- Batch normalization is usually applied with mini-batches.\\n- If we are using batch normalization parameters `b[1]`, ..., `b[L]` doesn't count because they will be eliminated after mean subtraction step, so:\\n  ```\\n  Z[l] = W[l]A[l-1] + b[l] => Z[l] = W[l]A[l-1]\\n  Z_norm[l] = ...\\n  Z_tilde[l] = gamma[l] * Z_norm[l] + beta[l]\\n  ```\\n  - Taking the mean of a constant `b[l]` will eliminate the `b[l]`\\n- So if you are using batch normalization, you can remove b[l] or make it always zero.\\n- So the parameters will be `W[l]`, `beta[l]`, and `alpha[l]`.\\n- Shapes:\\n  - `Z[l]       - (n[l], m)`\\n  - `beta[l]    - (n[l], m)`\\n  - `gamma[l]   - (n[l], m)`\\n### Why does Batch normalization work?\\n- The first reason is the same reason as why we normalize X.\\n- The second reason is that batch normalization reduces the problem of input values changing (shifting).\\n- Batch normalization does some regularization:\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- Batch normalization does some regularization:\\n  - Each mini batch is scaled by the mean/variance computed of that mini-batch.\\n  - This adds some noise to the values `Z[l]` within that mini batch. So similar to dropout it adds some noise to each hidden layer's activations.\\n  - This has a slight regularization effect.\\n  - Using bigger size of the mini-batch you are reducing noise and therefore regularization effect.\\n  - Don't rely on batch normalization as a regularization. It's intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques (L2 or dropout).\\n### Batch normalization at test time\\n- When we train a NN with Batch normalization, we compute the mean and the variance of the mini-batch.\\n- In testing we might need to process examples one at a time. The mean and the variance of one example won't make sense.\\n- We have to compute an estimated value of mean and variance to use it in testing time.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- We have to compute an estimated value of mean and variance to use it in testing time.\\n- We can use the weighted average across the mini-batches.\\n- We will use the estimated values of the mean and variance to test.\\n- This method is also sometimes called \"Running average\".\\n- In practice most often you will use a deep learning framework and it will contain some default implementation of doing such a thing.\\n### Softmax Regression\\n- In every example we have used so far we were talking about binary classification.\\n- There are a generalization of logistic regression called Softmax regression that is used for multiclass classification/regression.\\n- For example if we are classifying by classes `dog`, `cat`, `baby chick` and `none of that`\\n  - Dog `class = 1`\\n  - Cat `class = 2`\\n  - Baby chick `class = 3`\\n  - None `class = 0`\\n  - To represent a dog vector `y = [0 1 0 0]`\\n  - To represent a cat vector `y = [0 0 1 0]`\\n  - To represent a baby chick vector `y = [0 0 0 1]`'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- To represent a dog vector `y = [0 1 0 0]`\\n  - To represent a cat vector `y = [0 0 1 0]`\\n  - To represent a baby chick vector `y = [0 0 0 1]`\\n  - To represent a none vector `y = [1 0 0 0]`\\n- Notations:\\n  - `C = no. of classes`\\n  - Range of classes is `(0, ..., C-1)`\\n  - In output layer `Ny = C`\\n- Each of C values in the output layer will contain a probability of the example to belong to each of the classes.\\n- In the last layer we will have to activate the Softmax activation function instead of the sigmoid activation.\\n- Softmax activation equations:\\n  ```\\n  t = e^(Z[L])                      # shape(C, m)\\n  A[L] = e^(Z[L]) / sum(t)          # shape(C, m), sum(t) - sum of t's for each example (shape (1, m))\\n  ```\\n### Training a Softmax classifier\\n- There's an activation which is called hard max, which gets 1 for the maximum value and zeros for the others.\\n  - If you are using NumPy, its `np.max` over the vertical axis.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"- If you are using NumPy, its `np.max` over the vertical axis.\\n- The Softmax name came from softening the values and not harding them like hard max.\\n- Softmax is a generalization of logistic activation function to `C` classes. If `C = 2` softmax reduces to logistic regression.\\n- The loss function used with softmax:\\n  ```\\n  L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1\\n  ```\\n- The cost function used with softmax:\\n  ```\\n  J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m\\n  ```\\n- Back propagation with softmax:\\n  ```\\n  dZ[L] = Y_hat - Y\\n  ```\\n- The derivative of softmax is:\\n  ```\\n  Y_hat * (1 - Y_hat)\\n  ```\\n- Example:\\n    ![](Images/07-_softmax.png)\\n### Deep learning frameworks\\n- It's not practical to implement everything from scratch. Our numpy implementations were to know how NN works.\\n- There are many good deep learning frameworks.\\n- Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on going.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on going.\\n- Here are some of the leading deep learning frameworks:\\n  - Caffe/ Caffe2\\n  - CNTK\\n  - DL4j\\n  - Keras\\n  - Lasagne\\n  - mxnet\\n  - PaddlePaddle\\n  - TensorFlow\\n  - Theano\\n  - Torch/Pytorch\\n- These frameworks are getting better month by month. Comparison between them can be found [here](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\\n- How to choose deep learning framework:\\n  - Ease of programming (development and deployment)\\n  - Running speed\\n  - Truly open (open source with good governance)\\n- Programming frameworks can not only shorten your coding time but sometimes also perform optimizations that speed up your code.\\n### TensorFlow\\n- In this section we will learn the basic structure of TensorFlow programs.\\n- Lets see how to implement a minimization function:\\n  - Example function: `J(w) = w^2 - 10w + 25`'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- Lets see how to implement a minimization function:\\n  - Example function: `J(w) = w^2 - 10w + 25`\\n  - The result should be `w = 5` as the function is `(w-5)^2 = 0`\\n  - Code v.1:\\n    ```python\\n    import numpy as np\\n    import tensorflow as tf\\n    \\n    \\n    w = tf.Variable(0, dtype=tf.float32)                 # creating a variable w\\n    cost = tf.add(tf.add(w**2, tf.multiply(-10.0, w)), 25.0)        # can be written as this - cost = w**2 - 10*w + 25\\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\\n    init = tf.global_variables_initializer()\\n    session = tf.Session()\\n    session.run(init)\\n    session.run(w)    # Runs the definition of w, if you print this it will print zero\\n    session.run(train)\\n    print(\"W after one iteration:\", session.run(w))\\n    for i in range(1000):\\n    \\tsession.run(train)\\n    print(\"W after 1000 iterations:\", session.run(w))\\n    ```\\n  - Code v.2 (we feed the inputs to the algorithm through coefficients):\\n    ```python\\n    import numpy as np'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='```\\n  - Code v.2 (we feed the inputs to the algorithm through coefficients):\\n    ```python\\n    import numpy as np\\n    import tensorflow as tf\\n    \\n    \\n    coefficients = np.array([[1.], [-10.], [25.]])\\n    x = tf.placeholder(tf.float32, [3, 1])\\n    w = tf.Variable(0, dtype=tf.float32)                 # Creating a variable w\\n    cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\\n    init = tf.global_variables_initializer()\\n    session = tf.Session()\\n    session.run(init)\\n    session.run(w)    # Runs the definition of w, if you print this it will print zero\\n    session.run(train, feed_dict={x: coefficients})\\n    print(\"W after one iteration:\", session.run(w))\\n    for i in range(1000):\\n    \\tsession.run(train, feed_dict={x: coefficients})\\n    print(\"W after 1000 iterations:\", session.run(w))\\n    ```\\n- In TensorFlow you implement only the forward propagation and TensorFlow will do the backpropagation by itself.'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content=\"```\\n- In TensorFlow you implement only the forward propagation and TensorFlow will do the backpropagation by itself.\\n- In TensorFlow a placeholder is a variable you can assign a value to later.\\n- If you are using a mini-batch training you should change the `feed_dict={x: coefficients}` to the current mini-batch data.\\n- Almost all TensorFlow programs use this:\\n  ```python\\n  with tf.Session() as session:       # better for cleaning up in case of error/exception\\n  \\tsession.run(init)\\n  \\tsession.run(w)\\n  ```\\n- In deep learning frameworks there are a lot of things that you can do with one line of code like changing the optimizer.\\n_**Side notes:**_\\n- Writing and running programs in TensorFlow has the following steps:\\n  1. Create Tensors (variables) that are not yet executed/evaluated.\\n  2. Write operations between those Tensors.\\n  3. Initialize your Tensors.\\n  4. Create a Session.\\n  5. Run the Session. This will run the operations you'd written above.\"), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='3. Initialize your Tensors.\\n  4. Create a Session.\\n  5. Run the Session. This will run the operations you\\'d written above.\\n- Instead of needing to write code to compute the cost function we know, we can use this line in TensorFlow :\\n  `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\\n- To initialize weights in NN using TensorFlow use:\\n  ```\\n  W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\\n  b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\\n  ```\\n- For 3-layer NN, it is important to note that the forward propagation stops at `Z3`. The reason is that in TensorFlow the last linear layer output is given as input to the function computing the loss. Therefore, you don\\'t need `A3`!\\n- To reset the graph use `tf.reset_default_graph()`\\n## Extra Notes'), Document(metadata={'source': 'docs\\\\Readme (3).md'}, page_content='- To reset the graph use `tf.reset_default_graph()`\\n## Extra Notes\\n- If you want a good papers in deep learning look at the ICLR proceedings (Or NIPS proceedings) and that will give you a really good view of the field.\\n- Who is Yuanqing Lin?\\n  - Head of Baidu research.\\n  - First one to win ImageNet\\n  - Works in PaddlePaddle deep learning platform.\\n<br><br>\\n<br><br>\\nThese Notes were made by [Mahmoud Badry](mailto:mma18@fayoum.edu.eg) @2017'), Document(metadata={'source': 'docs\\\\README.md'}, page_content=\"# Deep Learning Specialization on Coursera (offered by deeplearning.ai)\\nProgramming assignments and quizzes from all courses in the Coursera [Deep Learning specialization](https://www.coursera.org/specializations/deep-learning) offered by `deeplearning.ai`.\\nInstructor: [Andrew Ng](http://www.andrewng.org/)\\n## Notes\\n### For detailed interview-ready notes on all courses in the Coursera Deep Learning specialization, refer [www.aman.ai](https://aman.ai/).\\n## Setup\\nRun ```setup.sh``` to (i) download a pre-trained VGG-19 dataset and (ii) extract the zip'd pre-trained models and datasets that are needed for all the assignments.\\n## Credits\\nThis repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning), unless specified otherwise.\\n## 2021 Version\"), Document(metadata={'source': 'docs\\\\README.md'}, page_content='## 2021 Version\\nThis specialization was updated in April 2021 to include developments in deep learning and programming frameworks, with the biggest change being shifting from TensorFlow 1 to TensorFlow 2. This repo has been updated accordingly as well.\\n## Programming Assignments\\n### Course 1: Neural Networks and Deep Learning\\n  - [Week 2 - PA 1 - Python Basics with Numpy](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb)\\n  - [Week 2 - PA 2 - Logistic Regression with a Neural Network mindset](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 3 - PA 3 - Planar data classification with one hidden layer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb)\\n  - [Week 4 - PA 4 - Building your Deep Neural Network: Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 4 - PA 5 - Deep Neural Network for Image Classification: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb)\\n### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\\n  - [Week 1 - PA 1 - Initialization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 1 - PA 2 - Regularization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb)\\n  - [Week 1 - PA 3 - Gradient Checking](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb)\\n  - [Week 2 - PA 4 - Optimization Methods](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 3 - PA 5 - TensorFlow Tutorial](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Tensorflow_introduction.ipynb)\\n### Course 3: Structuring Machine Learning Projects\\n  - There are no programming assignments for this course. But this course comes with very interesting case study quizzes (below).\\n  \\n### Course 4: Convolutional Neural Networks\\n  - [Week 1 - PA 1 - Convolutional Model: step by step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v1.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 1 - PA 2 - Convolutional Neural Networks: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application.ipynb)\\n  - [Week 2 - PA 1 - Keras - Tutorial - Happy House](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb)\\n  - [Week 2 - PA 2 - Residual Networks](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 2 - PA 2 - Transfer Learning with MobileNet](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Transfer%20Learning%20with%20MobileNet/Transfer_learning_with_MobileNet_v1.ipynb)\\n  - [Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection.ipynb)\\n  - [Week 3 - PA 2 - Image Segmentation Unet](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Image%20Segmentation%20Unet/Image_segmentation_Unet_v2.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 4 - PA 1 - Art Generation with Neural Style Transfer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb)    \\n  - [Week 4 - PA 2 - Face Recognition](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition.ipynb)\\n  \\n### Course 5: Sequence Models\\n  - [Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model.ipynb)\\n  - [Week 1 - PA 3 - Jazz improvisation with LSTM](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4_Solution.ipynb)  \\n  - [Week 2 - PA 1 - Word Vector Representation and Debiasing](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 2 - PA 2 - Emojify!](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emoji_v3a.ipynb)  \\n  - [Week 3 - PA 1 - Neural Machine Translation with Attention](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb)  \\n  - [Week 3 - PA 2 - Trigger Word Detection](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v2a.ipynb)\\n  - [Week 4 - PA 1 - Transformer Network](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%204/Transformer%20Subclass/C5_W4_A1_Transformer_Subclass_v1.ipynb)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- [Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Named%20Entity%20Recognition/Transformer_application_Named_Entity_Recognition.ipynb)   \\n  - [Week 3 - PA 2 - Transformer Network Application: Question Answering](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Question%20Answering/QA_transformer.ipynb) \\n  \\n## Quiz Solutions\\n### Course 1: Neural Networks and Deep Learning'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='## Quiz Solutions\\n### Course 1: Neural Networks and Deep Learning\\n  - Week 1 Quiz - Introduction to deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 2 Quiz - Neural Network Basics: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf)\\n  - Week 3 Quiz - Shallow Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 4 Quiz - Key concepts on Deep Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf)\\n### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\\n  - Week 1 Quiz - Practical aspects of deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 2 Quiz - Optimization algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf)\\n  \\n### Course 3: Structuring Machine Learning Projects'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='### Course 3: Structuring Machine Learning Projects\\n  - Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 2 Quiz - Autonomous driving (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf)\\n### Course 4: Convolutional Neural Networks'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='### Course 4: Convolutional Neural Networks\\n  - Week 1 Quiz - The basics of ConvNets: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf)\\n  - Week 2 Quiz - Deep convolutional models: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 3 Quiz - Detection algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 4 Quiz - Special applications: Face recognition & Neural style transfer: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf)\\n### Course 5: Sequence Models'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='### Course 5: Sequence Models\\n  - Week 1 Quiz - Recurrent Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf)\\n  - Week 2 Quiz - Natural Language Processing & Word Embeddings: [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf)'), Document(metadata={'source': 'docs\\\\README.md'}, page_content='- Week 3 Quiz - Sequence models & Attention mechanism: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf)\\n## Disclaimer'), Document(metadata={'source': 'docs\\\\README.md'}, page_content=\"## Disclaimer\\nI recognize the time people spend on building intuition, understanding new concepts and debugging assignments. The solutions uploaded here are **only for reference**. They are meant to unblock you if you get stuck somewhere. Please do not copy any part of the code as-is (the programming assignments are fairly easy if you read the instructions carefully). Similarly, try out the quizzes yourself before you refer to the quiz solutions. This course is the most straight-forward deep learning course I have ever taken, with fabulous course content and structure. It's a treasure by the deeplearning.ai team.\")]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "loader = ArxivLoader(\"1706.03762\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 1,\n",
    "    chunk_overlap = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"foo bar assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' assignment']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}, page_content='Hi this is Jim  \\nHi this is Joe')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 2'}, page_content='Hi this is Molly')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NotionDirectoryLoader(\"docs/\")\n",
    "docs = loader.load()\n",
    "txt = ' '.join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on =  [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\")\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits = markdown_splitter.split_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Neural Networks and Deep Learning'}, page_content='This is the first course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "###########################################################\n",
      "page_content='This is the first course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.' metadata={'Header 1': 'Neural Networks and Deep Learning'}\n",
      "1\n",
      "###########################################################\n",
      "page_content='* [Neural Networks and Deep Learning](#neural-networks-and-deep-learning)\n",
      "* [Table of contents](#table-of-contents)\n",
      "* [Course summary](#course-summary)\n",
      "* [Introduction to deep learning](#introduction-to-deep-learning)\n",
      "* [What is a (Neural Network) NN?](#what-is-a-neural-network-nn)\n",
      "* [Supervised learning with neural networks](#supervised-learning-with-neural-networks)\n",
      "* [Why is deep learning taking off?](#why-is-deep-learning-taking-off)\n",
      "* [Neural Networks Basics](#neural-networks-basics)\n",
      "* [Binary classification](#binary-classification)\n",
      "* [Logistic regression](#logistic-regression)\n",
      "* [Logistic regression cost function](#logistic-regression-cost-function)\n",
      "* [Gradient Descent](#gradient-descent)\n",
      "* [Derivatives](#derivatives)\n",
      "* [More Derivatives examples](#more-derivatives-examples)\n",
      "* [Computation graph](#computation-graph)\n",
      "* [Derivatives with a Computation Graph](#derivatives-with-a-computation-graph)\n",
      "* [Logistic Regression Gradient Descent](#logistic-regression-gradient-descent)\n",
      "* [Gradient Descent on m Examples](#gradient-descent-on-m-examples)\n",
      "* [Vectorization](#vectorization)\n",
      "* [Vectorizing Logistic Regression](#vectorizing-logistic-regression)\n",
      "* [Notes on Python and NumPy](#notes-on-python-and-numpy)\n",
      "* [General Notes](#general-notes)\n",
      "* [Shallow neural networks](#shallow-neural-networks)\n",
      "* [Neural Networks Overview](#neural-networks-overview)\n",
      "* [Neural Network Representation](#neural-network-representation)\n",
      "* [Computing a Neural Network's Output](#computing-a-neural-networks-output)\n",
      "* [Vectorizing across multiple examples](#vectorizing-across-multiple-examples)\n",
      "* [Activation functions](#activation-functions)\n",
      "* [Why do you need non-linear activation functions?](#why-do-you-need-non-linear-activation-functions)\n",
      "* [Derivatives of activation functions](#derivatives-of-activation-functions)\n",
      "* [Gradient descent for Neural Networks](#gradient-descent-for-neural-networks)\n",
      "* [Random Initialization](#random-initialization)\n",
      "* [Deep Neural Networks](#deep-neural-networks)\n",
      "* [Deep L-layer neural network](#deep-l-layer-neural-network)\n",
      "* [Forward Propagation in a Deep Network](#forward-propagation-in-a-deep-network)\n",
      "* [Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)\n",
      "* [Why deep representations?](#why-deep-representations)\n",
      "* [Building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)\n",
      "* [Forward and Backward Propagation](#forward-and-backward-propagation)\n",
      "* [Parameters vs Hyperparameters](#parameters-vs-hyperparameters)\n",
      "* [What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)\n",
      "* [Extra: Ian Goodfellow interview](#extra-ian-goodfellow-interview)' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Table of contents'}\n",
      "2\n",
      "###########################################################\n",
      "page_content='Here are the course summary as its given on the course [link](https://www.coursera.org/learn/neural-networks-deep-learning):  \n",
      "> If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new \"superpower\" that will let you build AI systems that just weren't possible a few years ago.\n",
      ">\n",
      "> In this course, you will learn the foundations of deep learning. When you finish this class, you will:\n",
      "> - Understand the major technology trends driving Deep Learning\n",
      "> - Be able to build, train and apply fully connected deep neural networks\n",
      "> - Know how to implement efficient (vectorized) neural networks\n",
      "> - Understand the key parameters in a neural network's architecture\n",
      ">\n",
      "> This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions.' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Course summary'}\n",
      "3\n",
      "###########################################################\n",
      "page_content='> Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.  \n",
      "### What is a (Neural Network) NN?  \n",
      "- Single neuron == linear regression without applying activation(perceptron)\n",
      "- Basically a single neuron will calculate weighted sum of input(W.T*X) and then we can set a threshold to predict output in a perceptron. If weighted sum of input cross the threshold, perceptron fires and if not then perceptron doesn't predict.\n",
      "- Perceptron can take real values input or boolean values.\n",
      "- Actually, when w⋅x+b=0 the perceptron outputs 0.\n",
      "- Disadvantage of perceptron is that it only output binary values and if we try to give small change in weight and bais then perceptron can flip the output. We need some system which can modify the output slightly according to small change in weight and bias. Here comes sigmoid function in picture.\n",
      "- If we change perceptron with a sigmoid function, then we can make slight change in output.\n",
      "- e.g. output in perceptron = 0, you slightly changed weight and bias, output becomes = 1 but actual output is 0.7. In case of sigmoid, output1 = 0, slight change in weight and bias, output = 0.7.\n",
      "- If we apply sigmoid activation function then Single neuron will act as Logistic Regression.\n",
      "-  we can understand difference between perceptron and sigmoid function by looking at sigmoid function graph.  \n",
      "- Simple NN graph:\n",
      "- ![](Images/Others/01.jpg)\n",
      "- Image taken from [tutorialspoint.com](http://www.tutorialspoint.com/)\n",
      "- RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.\n",
      "- Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.\n",
      "- Deep NN consists of more hidden layers (Deeper layers)\n",
      "- ![](Images/Others/02.png)\n",
      "- Image taken from [opennn.net](http://www.opennn.net/)\n",
      "- Each Input will be connected to the hidden layer and the NN will decide the connections.\n",
      "- Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.  \n",
      "### Supervised learning with neural networks  \n",
      "- Different types of neural networks for supervised learning which includes:\n",
      "- CNN or convolutional neural networks (Useful in computer vision)\n",
      "- RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\n",
      "- Standard NN (Useful for Structured data)\n",
      "- Hybrid/custom NN or a Collection of NNs types\n",
      "- Structured data is like the databases and tables.\n",
      "- Unstructured data is like images, video, audio, and text.\n",
      "- Structured data gives more money because companies relies on prediction on its big data.  \n",
      "### Why is deep learning taking off?  \n",
      "- Deep learning is taking off for 3 reasons:\n",
      "1. Data:\n",
      "- Using this image we can conclude:\n",
      "- ![](Images/11.png)\n",
      "- For small data NN can perform as Linear regression or SVM (Support vector machine)\n",
      "- For big data a small NN is better that SVM\n",
      "- For big data a big NN is better that a medium NN is better that small NN.\n",
      "- Hopefully we have a lot of data because the world is using the computer a little bit more\n",
      "- Mobiles\n",
      "- IOT (Internet of things)\n",
      "2. Computation:\n",
      "- GPUs.\n",
      "- Powerful CPUs.\n",
      "- Distributed computing.\n",
      "- ASICs\n",
      "3. Algorithm:\n",
      "1. Creative algorithms has appeared that changed the way NN works.\n",
      "- For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem.' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Introduction to deep learning'}\n",
      "4\n",
      "###########################################################\n",
      "page_content='> Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.  \n",
      "### Binary classification  \n",
      "- Mainly he is talking about how to do a logistic regression to make a binary classifier.\n",
      "- ![log](Images/Others/03.png)\n",
      "- Image taken from [3.bp.blogspot.com](http://3.bp.blogspot.com)\n",
      "- He talked about an example of knowing if the current image contains a cat or not.\n",
      "- Here are some notations:\n",
      "- `M is the number of training vectors`\n",
      "- `Nx is the size of the input vector`\n",
      "- `Ny is the size of the output vector`\n",
      "- `X(1) is the first input vector`\n",
      "- `Y(1) is the first output vector`\n",
      "- `X = [x(1) x(2).. x(M)]`\n",
      "- `Y = (y(1) y(2).. y(M))`\n",
      "- We will use python in this course.\n",
      "- In NumPy we can make matrices and make operations on them in a fast and reliable time.  \n",
      "### Logistic regression  \n",
      "- Algorithm is used for classification algorithm of 2 classes.\n",
      "- Equations:\n",
      "- Simple equation:`y = wx + b`\n",
      "- If x is a vector: `y = w(transpose)x + b`\n",
      "- If we need y to be in between 0 and 1 (probability): `y = sigmoid(w(transpose)x + b)`\n",
      "- In some notations this might be used: `y = sigmoid(w(transpose)x)`\n",
      "- While `b` is `w0` of `w` and we add `x0 = 1`. but we won't use this notation in the course (Andrew said that the first notation is better).\n",
      "- In binary classification `Y` has to be between `0` and `1`.\n",
      "- In the last equation `w` is a vector of `Nx` and `b` is a real number  \n",
      "### Logistic regression cost function  \n",
      "- First loss function would be the square root error:  `L(y',y) = 1/2 (y' - y)^2`\n",
      "- But we won't use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.\n",
      "- This is the function that we will use: `L(y',y) = - (y*log(y') + (1-y)*log(1-y'))`\n",
      "- To explain the last function lets see:\n",
      "- if `y = 1` ==> `L(y',1) = -log(y')`  ==> we want `y'` to be the largest   ==> `y`' biggest value is 1\n",
      "- if `y = 0` ==> `L(y',0) = -log(1-y')` ==> we want `1-y'` to be the largest ==> `y'` to be smaller as possible because it can only has 1 value.\n",
      "- Then the Cost function will be: `J(w,b) = (1/m) * Sum(L(y'[i],y[i]))`\n",
      "- The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  \n",
      "### Gradient Descent  \n",
      "- We want to predict `w` and `b` that minimize the cost function.\n",
      "- Our cost function is convex.\n",
      "- First we initialize `w` and `b` to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.\n",
      "- In Logistic regression people always use 0,0 instead of random.\n",
      "- The gradient decent algorithm repeats: `w = w - alpha * dw`\n",
      "where alpha is the learning rate and `dw` is the derivative of `w` (Change to `w`)\n",
      "The derivative is also the slope of `w`\n",
      "- Looks like greedy algorithms. the derivative give us the direction to improve our parameters.  \n",
      "- The actual equations we will implement:\n",
      "- `w = w - alpha * d(J(w,b) / dw)`        (how much the function slopes in the w direction)\n",
      "- `b = b - alpha * d(J(w,b) / db)`        (how much the function slopes in the d direction)  \n",
      "### Derivatives  \n",
      "- We will talk about some of required calculus.\n",
      "- You don't need to be a calculus geek to master deep learning but you'll need some skills from it.\n",
      "- Derivative of a linear line is its slope.\n",
      "- ex. `f(a) = 3a`                    `d(f(a))/d(a) = 3`\n",
      "- if `a = 2` then `f(a) = 6`\n",
      "- if we move a a little bit `a = 2.001` then `f(a) = 6.003` means that we multiplied the derivative (Slope) to the moved area and added it to the last result.  \n",
      "### More Derivatives examples  \n",
      "- `f(a) = a^2`  ==> `d(f(a))/d(a) = 2a`\n",
      "- `a = 2`  ==> `f(a) = 4`\n",
      "- `a = 2.0001` ==> `f(a) = 4.0004` approx.\n",
      "- `f(a) = a^3`  ==> `d(f(a))/d(a) = 3a^2`\n",
      "- `f(a) = log(a)`  ==> `d(f(a))/d(a) = 1/a`\n",
      "- To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.  \n",
      "### Computation graph  \n",
      "- Its a graph that organizes the computation from left to right.\n",
      "- ![](Images/02.png)  \n",
      "### Derivatives with a Computation Graph  \n",
      "- Calculus chain rule says:\n",
      "If `x -> y -> z`          (x effect y and y effects z)\n",
      "Then `d(z)/d(x) = d(z)/d(y) * d(y)/d(x)`\n",
      "- The video illustrates a big example.\n",
      "- ![](Images/03.png)\n",
      "- We compute the derivatives on a graph from right to left and it will be a lot more easier.\n",
      "- `dvar` means the derivatives of a final output variable with respect to various intermediate quantities.  \n",
      "### Logistic Regression Gradient Descent  \n",
      "- In the video he discussed the derivatives of gradient decent example for one sample with two features `x1` and `x2`.\n",
      "- ![](Images/04.png)  \n",
      "### Gradient Descent on m Examples  \n",
      "- Lets say we have these variables:  \n",
      "```\n",
      "X1                  Feature\n",
      "X2                  Feature\n",
      "W1                  Weight of the first feature.\n",
      "W2                  Weight of the second feature.\n",
      "B                   Logistic Regression parameter.\n",
      "M                   Number of training examples\n",
      "Y(i)                Expected output of i\n",
      "```  \n",
      "- So we have:\n",
      "![](Images/09.png)  \n",
      "- Then from right to left we will calculate derivations compared to the result:  \n",
      "```\n",
      "d(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))\n",
      "d(z)  = d(l)/d(z) = a - y\n",
      "d(W1) = X1 * d(z)\n",
      "d(W2) = X2 * d(z)\n",
      "d(B)  = d(z)\n",
      "```  \n",
      "- From the above we can conclude the logistic regression pseudo code:  \n",
      "```\n",
      "J = 0; dw1 = 0; dw2 =0; db = 0;                 # Devs.\n",
      "w1 = 0; w2 = 0; b=0;# Weights\n",
      "for i = 1 to m\n",
      "# Forward pass\n",
      "z(i) = W1*x1(i) + W2*x2(i) + b\n",
      "a(i) = Sigmoid(z(i))\n",
      "J += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))\n",
      "\n",
      "# Backward pass\n",
      "dz(i) = a(i) - Y(i)\n",
      "dw1 += dz(i) * x1(i)\n",
      "dw2 += dz(i) * x2(i)\n",
      "db  += dz(i)\n",
      "J /= m\n",
      "dw1/= m\n",
      "dw2/= m\n",
      "db/= m\n",
      "\n",
      "# Gradient descent\n",
      "w1 = w1 - alpha * dw1\n",
      "w2 = w2 - alpha * dw2\n",
      "b = b - alpha * db\n",
      "```  \n",
      "- The above code should run for some iterations to minimize error.  \n",
      "- So there will be two inner loops to implement the logistic regression.  \n",
      "- Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!  \n",
      "### Vectorization  \n",
      "- Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why we need vectorization to get rid of some of our for loops.\n",
      "- NumPy library (dot) function is using vectorization by default.\n",
      "- The vectorization can be done on CPU or GPU thought the SIMD operation. But its faster on GPU.\n",
      "- Whenever possible avoid for loops.\n",
      "- Most of the NumPy library methods are vectorized version.  \n",
      "### Vectorizing Logistic Regression  \n",
      "- We will implement Logistic Regression using one for loop then without any for loop.\n",
      "- As an input we have a matrix `X` and its `[Nx, m]` and a matrix `Y` and its `[Ny, m]`.\n",
      "- We will then compute at instance `[z1,z2...zm] = W' * X + [b,b,...b]`. This can be written in python as:  \n",
      "Z = np.dot(W.T,X) + b    # Vectorization, then broadcasting, Z shape is (1, m)\n",
      "A = 1 / 1 + np.exp(-Z)   # Vectorization, A shape is (1, m)  \n",
      "- Vectorizing Logistic Regression's Gradient Output:  \n",
      "dz = A - Y                  # Vectorization, dz shape is (1, m)\n",
      "dw = np.dot(X, dz.T) / m    # Vectorization, dw shape is (Nx, 1)\n",
      "db = dz.sum() / m           # Vectorization, dz shape is (1, 1)  \n",
      "### Notes on Python and NumPy  \n",
      "- In NumPy, `obj.sum(axis = 0)` sums the columns while `obj.sum(axis = 1)` sums the rows.\n",
      "- In NumPy, `obj.reshape(1,4)` changes the shape of the matrix by broadcasting the values.\n",
      "- Reshape is cheap in calculations so put it everywhere you're not sure about the calculations.\n",
      "- Broadcasting works when you do a matrix operation with matrices that doesn't match for the operation, in this case NumPy automatically makes the shapes ready for the operation by broadcasting the values.\n",
      "- In general principle of broadcasting. If you have an (m,n) matrix and you add(+) or subtract(-) or multiply(*) or divide(/) with a (1,n) matrix, then this will copy it m times into an (m,n) matrix. The same with if you use those operations with a (m , 1) matrix, then this will copy it n times into (m, n) matrix. And then apply the addition, subtraction, and multiplication of division element wise.\n",
      "- Some tricks to eliminate all the strange bugs in the code:\n",
      "- If you didn't specify the shape of a vector, it will take a shape of `(m,)` and the transpose operation won't work. You have to reshape it to `(m, 1)`\n",
      "- Try to not use the rank one matrix in ANN\n",
      "- Don't hesitate to use `assert(a.shape == (5,1))` to check if your matrix shape is the required one.\n",
      "- If you've found a rank one matrix try to run reshape on it.\n",
      "- Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at the same time. It runs in the browser and doesn't need an IDE to run.\n",
      "- To open Jupyter Notebook, open the command line and call: `jupyter-notebook` It should be installed to work.\n",
      "- To Compute the derivative of Sigmoid:  \n",
      "```\n",
      "s = sigmoid(x)\n",
      "ds = s * (1 - s)       # derivative  using calculus\n",
      "```  \n",
      "- To make an image of `(width,height,depth)` be a vector, use this:  \n",
      "```\n",
      "v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)  #reshapes the image.\n",
      "```  \n",
      "- Gradient descent converges faster after normalization of the input matrices.  \n",
      "### General Notes  \n",
      "- The main steps for building a Neural Network are:\n",
      "- Define the model structure (such as number of input features and outputs)\n",
      "- Initialize the model's parameters.\n",
      "- Loop.\n",
      "- Calculate current loss (forward propagation)\n",
      "- Calculate current gradient (backward propagation)\n",
      "- Update parameters (gradient descent)\n",
      "- Preprocessing the dataset is important.\n",
      "- Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm.\n",
      "- [kaggle.com](kaggle.com) is a good place for datasets and competitions.\n",
      "- [Pieter Abbeel](https://www2.eecs.berkeley.edu/Faculty/Homepages/abbeel.html) is one of the best in deep reinforcement learning.' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Neural Networks Basics'}\n",
      "5\n",
      "###########################################################\n",
      "page_content='> Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.  \n",
      "### Neural Networks Overview  \n",
      "- In logistic regression we had:  \n",
      "```\n",
      "X1  \\\n",
      "X2   ==>  z = XW + B ==> a = Sigmoid(z) ==> l(a,Y)\n",
      "X3  /\n",
      "```  \n",
      "- In neural networks with one layer we will have:  \n",
      "```\n",
      "X1  \\\n",
      "X2   =>  z1 = XW1 + B1 => a1 = Sigmoid(z1) => z2 = a1W2 + B2 => a2 = Sigmoid(z2) => l(a2,Y)\n",
      "X3  /\n",
      "```  \n",
      "- `X` is the input vector `(X1, X2, X3)`, and `Y` is the output variable `(1x1)`\n",
      "- NN is stack of logistic regression objects.  \n",
      "### Neural Network Representation  \n",
      "- We will define the neural networks that has one hidden layer.\n",
      "- NN contains of input layers, hidden layers, output layers.\n",
      "- Hidden layer means we cant see that layers in the training set.\n",
      "- `a0 = x` (the input layer)\n",
      "- `a1` will represent the activation of the hidden neurons.\n",
      "- `a2` will represent the output layer.\n",
      "- We are talking about 2 layers NN. The input layer isn't counted.  \n",
      "### Computing a Neural Network's Output  \n",
      "- Equations of Hidden layers:\n",
      "- ![](Images/05.png)\n",
      "- Here are some informations about the last image:\n",
      "- `noOfHiddenNeurons = 4`\n",
      "- `Nx = 3`\n",
      "- Shapes of the variables:\n",
      "- `W1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,nx)`\n",
      "- `b1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,1)`\n",
      "- `z1` is the result of the equation `z1 = W1*X + b`, it has a shape of `(noOfHiddenNeurons,1)`\n",
      "- `a1` is the result of the equation `a1 = sigmoid(z1)`, it has a shape of `(noOfHiddenNeurons,1)`\n",
      "- `W2` is the matrix of the second hidden layer, it has a shape of `(1,noOfHiddenNeurons)`\n",
      "- `b2` is the matrix of the second hidden layer, it has a shape of `(1,1)`\n",
      "- `z2` is the result of the equation `z2 = W2*a1 + b`, it has a shape of `(1,1)`\n",
      "- `a2` is the result of the equation `a2 = sigmoid(z2)`, it has a shape of `(1,1)`  \n",
      "### Vectorizing across multiple examples  \n",
      "- Pseudo code for forward propagation for the 2 layers NN:  \n",
      "```\n",
      "for i = 1 to m\n",
      "z[1, i] = W1*x[i] + b1      # shape of z[1, i] is (noOfHiddenNeurons,1)\n",
      "a[1, i] = sigmoid(z[1, i])  # shape of a[1, i] is (noOfHiddenNeurons,1)\n",
      "z[2, i] = W2*a[1, i] + b2   # shape of z[2, i] is (1,1)\n",
      "a[2, i] = sigmoid(z[2, i])  # shape of a[2, i] is (1,1)\n",
      "```  \n",
      "- Lets say we have `X` on shape `(Nx,m)`. So the new pseudo code:  \n",
      "```\n",
      "Z1 = W1X + b1     # shape of Z1 (noOfHiddenNeurons,m)\n",
      "A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\n",
      "Z2 = W2A1 + b2    # shape of Z2 is (1,m)\n",
      "A2 = sigmoid(Z2)  # shape of A2 is (1,m)\n",
      "```  \n",
      "- If you notice always m is the number of columns.\n",
      "- In the last example we can call `X` = `A0`. So the previous step can be rewritten as:  \n",
      "```\n",
      "Z1 = W1A0 + b1    # shape of Z1 (noOfHiddenNeurons,m)\n",
      "A1 = sigmoid(Z1)  # shape of A1 (noOfHiddenNeurons,m)\n",
      "Z2 = W2A1 + b2    # shape of Z2 is (1,m)\n",
      "A2 = sigmoid(Z2)  # shape of A2 is (1,m)\n",
      "```  \n",
      "### Activation functions  \n",
      "- So far we are using sigmoid, but in some cases other functions can be a lot better.\n",
      "- Sigmoid can lead us to gradient decent problem where the updates are so low.\n",
      "- Sigmoid activation function range is [0,1]\n",
      "`A = 1 / (1 + np.exp(-z)) # Where z is the input matrix`\n",
      "- Tanh activation function range is [-1,1]   (Shifted version of sigmoid function)\n",
      "- In NumPy we can implement Tanh using one of these methods:\n",
      "`A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) # Where z is the input matrix`  \n",
      "Or\n",
      "`A = np.tanh(z)   # Where z is the input matrix`\n",
      "- It turns out that the tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer.\n",
      "- Sigmoid or Tanh function disadvantage is that if the input is too small or too high, the slope will be near zero which will cause us the gradient decent problem.\n",
      "- One of the popular activation functions that solved the slow gradient decent is the RELU function.\n",
      "`RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the slope remains linear.`\n",
      "- So here is some basic rule for choosing activation functions, if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.\n",
      "- Leaky RELU activation function different of RELU is that if the input is negative the slope will be so small. It works as RELU but most people uses RELU.\n",
      "`Leaky_RELU = max(0.01z,z)  #the 0.01 can be a parameter for your algorithm.`\n",
      "- In NN you will decide a lot of choices like:\n",
      "- No of hidden layers.\n",
      "- No of neurons in each hidden layer.\n",
      "- Learning rate.       (The most important parameter)\n",
      "- Activation functions.\n",
      "- And others..\n",
      "- It turns out there are no guide lines for that. You should try all activation functions for example.  \n",
      "### Why do you need non-linear activation functions?  \n",
      "- If we removed the activation function from our algorithm that can be called linear activation function.\n",
      "- Linear activation function will output linear activations\n",
      "- Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems)\n",
      "- You might use linear activation function in one place - in the output layer if the output is real numbers (regression problem). But even in this case if the output value is non-negative you could use RELU instead.  \n",
      "### Derivatives of activation functions  \n",
      "- Derivation of Sigmoid activation function:  \n",
      "```\n",
      "g(z)  = 1 / (1 + np.exp(-z))\n",
      "g'(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))\n",
      "g'(z) = g(z) * (1 - g(z))\n",
      "```  \n",
      "- Derivation of Tanh activation function:  \n",
      "```\n",
      "g(z)  = (e^z - e^-z) / (e^z + e^-z)\n",
      "g'(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2\n",
      "```  \n",
      "- Derivation of RELU activation function:  \n",
      "```\n",
      "g(z)  = np.maximum(0,z)\n",
      "g'(z) = { 0  if z < 0\n",
      "1  if z >= 0  }\n",
      "```  \n",
      "- Derivation of leaky RELU activation function:  \n",
      "```\n",
      "g(z)  = np.maximum(0.01 * z, z)\n",
      "g'(z) = { 0.01  if z < 0\n",
      "1     if z >= 0   }\n",
      "```  \n",
      "### Gradient descent for Neural Networks\n",
      "- In this section we will have the full back propagation of the neural network (Just the equations with no explanations).\n",
      "- Gradient descent algorithm:\n",
      "- NN parameters:\n",
      "- `n[0] = Nx`\n",
      "- `n[1] = NoOfHiddenNeurons`\n",
      "- `n[2] = NoOfOutputNeurons = 1`\n",
      "- `W1` shape is `(n[1],n[0])`\n",
      "- `b1` shape is `(n[1],1)`\n",
      "- `W2` shape is `(n[2],n[1])`\n",
      "- `b2` shape is `(n[2],1)`\n",
      "- Cost function `I =  I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))`\n",
      "- Then Gradient descent:  \n",
      "```\n",
      "Repeat:\n",
      "Compute predictions (y'[i], i = 0,...m)\n",
      "Get derivatives: dW1, db1, dW2, db2\n",
      "Update: W1 = W1 - LearningRate * dW1\n",
      "b1 = b1 - LearningRate * db1\n",
      "W2 = W2 - LearningRate * dW2\n",
      "b2 = b2 - LearningRate * db2\n",
      "```  \n",
      "- Forward propagation:  \n",
      "```\n",
      "Z1 = W1A0 + b1    # A0 is X\n",
      "A1 = g1(Z1)\n",
      "Z2 = W2A1 + b2\n",
      "A2 = Sigmoid(Z2)      # Sigmoid because the output is between 0 and 1\n",
      "```  \n",
      "- Backpropagation (derivations):\n",
      "```\n",
      "dZ2 = A2 - Y      # derivative of cost function we used * derivative of the sigmoid function\n",
      "dW2 = (dZ2 * A1.T) / m\n",
      "db2 = Sum(dZ2) / m\n",
      "dZ1 = (W2.T * dZ2) * g'1(Z1)  # element wise product (*)\n",
      "dW1 = (dZ1 * A0.T) / m   # A0 = X\n",
      "db1 = Sum(dZ1) / m\n",
      "# Hint there are transposes with multiplication because to keep dimensions correct\n",
      "```\n",
      "- How we derived the 6 equations of the backpropagation:\n",
      "![](Images/06.png)  \n",
      "### Random Initialization  \n",
      "- In logistic regression it wasn't important to initialize the weights randomly, while in NN we have to initialize them randomly.  \n",
      "- If we initialize all the weights with zeros in NN it won't work (initializing bias with zero is OK):\n",
      "- all hidden units will be completely identical (symmetric) - compute exactly the same function\n",
      "- on each gradient descent iteration all the hidden units will always update the same  \n",
      "- To solve this we initialize the W's with a small random numbers:  \n",
      "```\n",
      "W1 = np.random.randn((2,2)) * 0.01    # 0.01 to make it small enough\n",
      "b1 = np.zeros((2,1))                  # its ok to have b as zero, it won't get us to the symmetry breaking problem\n",
      "```  \n",
      "- We need small values because in sigmoid (or tanh), for example, if the weight is too large you are more likely to end up even at the very start of training with very large values of Z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue.  \n",
      "- Constant 0.01 is alright for 1 hidden layer networks, but if the NN is deep this number can be changed but it will always be a small number.' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Shallow neural networks'}\n",
      "6\n",
      "###########################################################\n",
      "page_content='> Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.  \n",
      "### Deep L-layer neural network  \n",
      "- Shallow NN is a NN with one or two layers.\n",
      "- Deep NN is a NN with three or more layers.\n",
      "- We will use the notation `L` to denote the number of layers in a NN.\n",
      "- `n[l]` is the number of neurons in a specific layer `l`.\n",
      "- `n[0]` denotes the number of neurons input layer. `n[L]` denotes the number of neurons in output layer.\n",
      "- `g[l]` is the activation function.\n",
      "- `a[l] = g[l](z[l])`\n",
      "- `w[l]` weights is used for `z[l]`\n",
      "- `x = a[0]`, `a[l] = y'`\n",
      "- These were the notation we will use for deep neural network.\n",
      "- So we have:\n",
      "- A vector `n` of shape `(1, NoOfLayers+1)`\n",
      "- A vector `g` of shape `(1, NoOfLayers)`\n",
      "- A list of different shapes `w` based on the number of neurons on the previous and the current layer.\n",
      "- A list of different shapes `b` based on the number of neurons on the current layer.  \n",
      "### Forward Propagation in a Deep Network  \n",
      "- Forward propagation general rule for one input:  \n",
      "```\n",
      "z[l] = W[l]a[l-1] + b[l]\n",
      "a[l] = g[l](a[l])\n",
      "```  \n",
      "- Forward propagation general rule for `m` inputs:  \n",
      "```\n",
      "Z[l] = W[l]A[l-1] + B[l]\n",
      "A[l] = g[l](A[l])\n",
      "```  \n",
      "- We can't compute the whole layers forward propagation without a for loop so its OK to have a for loop here.\n",
      "- The dimensions of the matrices are so important you need to figure it out.  \n",
      "### Getting your matrix dimensions right  \n",
      "- The best way to debug your matrices dimensions is by a pencil and paper.\n",
      "- Dimension of `W` is `(n[l],n[l-1])` . Can be thought by right to left.\n",
      "- Dimension of `b` is `(n[l],1)`\n",
      "- `dw` has the same shape as `W`, while `db` is the same shape as `b`\n",
      "- Dimension of `Z[l],` `A[l]`, `dZ[l]`, and `dA[l]`  is `(n[l],m)`  \n",
      "### Why deep representations?  \n",
      "- Why deep NN works well, we will discuss this question in this section.\n",
      "- Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relation with the previous layer. E.g.:\n",
      "- 1) Face recognition application:\n",
      "- Image ==> Edges ==> Face parts ==> Faces ==> desired face\n",
      "- 2) Audio recognition application:\n",
      "- Audio ==> Low level sound features like (sss,bb) ==> Phonemes ==> Words ==> Sentences\n",
      "- Neural Researchers think that deep neural networks \"think\" like brains (simple ==> complex)\n",
      "- Circuit theory and deep learning:\n",
      "- ![](Images/07.png)\n",
      "- When starting on an application don't start directly by dozens of hidden layers. Try the simplest solutions (e.g. Logistic Regression), then try the shallow neural network and so on.  \n",
      "### Building blocks of deep neural networks  \n",
      "- Forward and back propagation for a layer l:\n",
      "- ![Untitled](Images/10.png)\n",
      "- Deep NN blocks:\n",
      "- ![](Images/08.png)  \n",
      "### Forward and Backward Propagation  \n",
      "- Pseudo code for forward propagation for layer l:  \n",
      "```\n",
      "Input  A[l-1]\n",
      "Z[l] = W[l]A[l-1] + b[l]\n",
      "A[l] = g[l](Z[l])\n",
      "Output A[l], cache(Z[l])\n",
      "```  \n",
      "- Pseudo  code for back propagation for layer l:  \n",
      "```\n",
      "Input da[l], Caches\n",
      "dZ[l] = dA[l] * g'[l](Z[l])\n",
      "dW[l] = (dZ[l]A[l-1].T) / m\n",
      "db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True\n",
      "dA[l-1] = w[l].T * dZ[l]            # The multiplication here are a dot product.\n",
      "Output dA[l-1], dW[l], db[l]\n",
      "```  \n",
      "- If we have used our loss function then:  \n",
      "```\n",
      "dA[L] = (-(y/a) + ((1-y)/(1-a)))\n",
      "```  \n",
      "### Parameters vs Hyperparameters  \n",
      "- Main parameters of the NN is `W` and `b`\n",
      "- Hyper parameters (parameters that control the algorithm) are like:\n",
      "- Learning rate.\n",
      "- Number of iteration.\n",
      "- Number of hidden layers `L`.\n",
      "- Number of hidden units `n`.\n",
      "- Choice of activation functions.\n",
      "- You have to try values yourself of hyper parameters.\n",
      "- In the earlier days of DL and ML learning rate was often called a parameter, but it really is (and now everybody call it) a hyperparameter.\n",
      "- On the next course we will see how to optimize hyperparameters.  \n",
      "### What does this have to do with the brain  \n",
      "- The analogy that \"It is like the brain\" has become really an oversimplified explanation.\n",
      "- There is a very simplistic analogy between a single logistic unit and a single neuron in the brain.\n",
      "- No human today understand how a human brain neuron works.\n",
      "- No human today know exactly how many neurons on the brain.\n",
      "- Deep learning in Andrew's opinion is very good at learning very flexible, complex functions to learn X to Y mappings, to learn input-output mappings (supervised learning).\n",
      "- The field of computer vision has taken a bit more inspiration from the human brains then other disciplines that also apply deep learning.\n",
      "- NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN)' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Deep Neural Networks'}\n",
      "7\n",
      "###########################################################\n",
      "page_content='- Ian is one of the world's most visible deep learning researchers.\n",
      "- Ian is mainly working with generative models. He is the creator of GANs.\n",
      "- We need to stabilize GANs. Stabilized GANs can become the best generative models.\n",
      "- Ian wrote the first textbook on the modern version of deep learning with Yoshua Bengio and Aaron Courville.\n",
      "- Ian worked with [OpenAI.com](https://openai.com/) and Google on ML and NN applications.\n",
      "- Ian tells all who wants to get into AI to get a Ph.D. or post your code on Github and the companies will find you.\n",
      "- Ian thinks that we need to start anticipating security problems with ML now and make sure that these algorithms are secure from the start instead of trying to patch it in retroactively years later.  \n",
      "<br><br>\n",
      "<br><br>\n",
      "These Notes were made by [Mahmoud Badry](mailto:mma18@fayoum.edu.eg) @2017' metadata={'Header 1': 'Neural Networks and Deep Learning', 'Header 2': 'Extra: Ian Goodfellow interview'}\n",
      "8\n",
      "###########################################################\n",
      "page_content='This is the second course of the deep learning specialization at [Coursera](https://www.coursera.org/specializations/deep-learning) which is moderated by [DeepLearning.ai](http://deeplearning.ai/). The course is taught by Andrew Ng.' metadata={'Header 1': 'Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization'}\n",
      "9\n",
      "###########################################################\n",
      "page_content='* [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](#improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization)\n",
      "* [Table of contents](#table-of-contents)\n",
      "* [Course summary](#course-summary)\n",
      "* [Practical aspects of Deep Learning](#practical-aspects-of-deep-learning)\n",
      "* [Train / Dev / Test sets](#train--dev--test-sets)\n",
      "* [Bias / Variance](#bias--variance)\n",
      "* [Basic Recipe for Machine Learning](#basic-recipe-for-machine-learning)\n",
      "* [Regularization](#regularization)\n",
      "* [Why regularization reduces overfitting?](#why-regularization-reduces-overfitting)\n",
      "* [Dropout Regularization](#dropout-regularization)\n",
      "* [Understanding Dropout](#understanding-dropout)\n",
      "* [Other regularization methods](#other-regularization-methods)\n",
      "* [Normalizing inputs](#normalizing-inputs)\n",
      "* [Vanishing / Exploding gradients](#vanishing--exploding-gradients)\n",
      "* [Weight Initialization for Deep Networks](#weight-initialization-for-deep-networks)\n",
      "* [Numerical approximation of gradients](#numerical-approximation-of-gradients)\n",
      "* [Gradient checking implementation notes](#gradient-checking-implementation-notes)\n",
      "* [Initialization summary](#initialization-summary)\n",
      "* [Regularization summary](#regularization-summary)\n",
      "* [Optimization algorithms](#optimization-algorithms)\n",
      "* [Mini-batch gradient descent](#mini-batch-gradient-descent)\n",
      "* [Understanding mini-batch gradient descent](#understanding-mini-batch-gradient-descent)\n",
      "* [Exponentially weighted averages](#exponentially-weighted-averages)\n",
      "* [Understanding exponentially weighted averages](#understanding-exponentially-weighted-averages)\n",
      "* [Bias correction in exponentially weighted averages](#bias-correction-in-exponentially-weighted-averages)\n",
      "* [Gradient descent with momentum](#gradient-descent-with-momentum)\n",
      "* [RMSprop](#rmsprop)\n",
      "* [Adam optimization algorithm](#adam-optimization-algorithm)\n",
      "* [Learning rate decay](#learning-rate-decay)\n",
      "* [The problem of local optima](#the-problem-of-local-optima)\n",
      "* [Hyperparameter tuning, Batch Normalization and Programming Frameworks](#hyperparameter-tuning-batch-normalization-and-programming-frameworks)\n",
      "* [Tuning process](#tuning-process)\n",
      "* [Using an appropriate scale to pick hyperparameters](#using-an-appropriate-scale-to-pick-hyperparameters)\n",
      "* [Hyperparameters tuning in practice: Pandas vs. Caviar](#hyperparameters-tuning-in-practice-pandas-vs-caviar)\n",
      "* [Normalizing activations in a network](#normalizing-activations-in-a-network)\n",
      "* [Fitting Batch Normalization into a neural network](#fitting-batch-normalization-into-a-neural-network)\n",
      "* [Why does Batch normalization work?](#why-does-batch-normalization-work)\n",
      "* [Batch normalization at test time](#batch-normalization-at-test-time)\n",
      "* [Softmax Regression](#softmax-regression)\n",
      "* [Training a Softmax classifier](#training-a-softmax-classifier)\n",
      "* [Deep learning frameworks](#deep-learning-frameworks)\n",
      "* [TensorFlow](#tensorflow)\n",
      "* [Extra Notes](#extra-notes)' metadata={'Header 1': 'Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization', 'Header 2': 'Table of contents'}\n",
      "10\n",
      "###########################################################\n",
      "page_content='Here are the course summary as its given on the course [link](https://www.coursera.org/learn/deep-neural-network):  \n",
      "> This course will teach you the \"magic\" of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow.\n",
      ">\n",
      "> After 3 weeks, you will:\n",
      "> - Understand industry best-practices for building deep learning applications.\n",
      "> - Be able to effectively use the common neural network \"tricks\", including initialization, L2 and dropout regularization, Batch normalization, gradient checking,\n",
      "> - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence.\n",
      "> - Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance\n",
      "> - Be able to implement a neural network in TensorFlow.\n",
      ">\n",
      "> This is the second course of the Deep Learning Specialization.' metadata={'Header 1': 'Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization', 'Header 2': 'Course summary'}\n",
      "11\n",
      "###########################################################\n",
      "page_content='### Train / Dev / Test sets  \n",
      "- Its impossible to get all your hyperparameters right on a new application from the first time.\n",
      "- So the idea is you go through the loop: `Idea ==> Code ==> Experiment`.\n",
      "- You have to go through the loop many times to figure out your hyperparameters.\n",
      "- Your data will be split into three parts:\n",
      "- Training set.       (Has to be the largest set)\n",
      "- Hold-out cross validation set / Development or \"dev\" set.\n",
      "- Testing set.\n",
      "- You will try to build a model upon training set then try to optimize hyperparameters on dev set as much as possible. Then after your model is ready you try and evaluate the testing set.\n",
      "- so the trend on the ratio of splitting the models:\n",
      "- If size of the  dataset is 100 to 1000000  ==> 60/20/20\n",
      "- If size of the  dataset is 1000000  to INF  ==> 98/1/1 or  99.5/0.25/0.25\n",
      "- The trend now gives the training data the biggest sets.\n",
      "- Make sure the dev and test set are coming from the same distribution.\n",
      "- For example if cat training/dev pictures are from the web but the test pictures are from users cell phone they will mismatch. It is better to make sure that dev and test set are from the same distribution.\n",
      "- The dev set rule is to try them on some of the good models you've created.\n",
      "- Its OK to only have a dev set without a testing set. But a lot of people in this case call the dev set as the test set. A better terminology is to call it a dev set as its used in the development.  \n",
      "### Bias / Variance  \n",
      "- Bias / Variance techniques are Easy to learn, but difficult to master.\n",
      "- So here the explanation of Bias / Variance:\n",
      "- If your model is underfitting (logistic regression of non linear data) it has a \"high bias\"\n",
      "- If your model is overfitting then it has a \"high variance\"\n",
      "- Your model will be alright if you balance the Bias / Variance\n",
      "- For more:\n",
      "- ![](Images/01-_Bias_-_Variance.png)\n",
      "- Another idea to get the bias /  variance if you don't have a 2D plotting mechanism:\n",
      "- High variance (overfitting) for example:\n",
      "- Training error: 1%\n",
      "- Dev error: 11%\n",
      "- high Bias (underfitting) for example:\n",
      "- Training error: 15%\n",
      "- Dev error: 14%\n",
      "- high Bias (underfitting) && High variance (overfitting) for example:\n",
      "- Training error: 15%\n",
      "- Test error: 30%\n",
      "- Best:\n",
      "- Training error: 0.5%\n",
      "- Test error: 1%\n",
      "- These Assumptions came from that human has 0% error. If the problem isn't like that you'll need to use human error as baseline.  \n",
      "### Basic Recipe for Machine Learning  \n",
      "- If your algorithm has a high bias:\n",
      "- Try to make your NN bigger (size of hidden units, number of layers)\n",
      "- Try a different model that is suitable for your data.\n",
      "- Try to run it longer.\n",
      "- Different (advanced) optimization algorithms.\n",
      "- If your algorithm has a high variance:\n",
      "- More data.\n",
      "- Try regularization.\n",
      "- Try a different model that is suitable for your data.\n",
      "- You should try the previous two points until you have a low bias and low variance.\n",
      "- In the older days before deep learning, there was a \"Bias/variance tradeoff\". But because now you have more options/tools for solving the bias and variance problem its really helpful to use deep learning.\n",
      "- Training a bigger neural network never hurts.  \n",
      "### Regularization  \n",
      "- Adding regularization to NN will help it reduce variance (overfitting)\n",
      "- L1 matrix norm:\n",
      "- `||W|| = Sum(|w[i,j]|)  # sum of absolute values of all w`\n",
      "- L2 matrix norm because of arcane technical math reasons is called Frobenius norm:\n",
      "- `||W||^2 = Sum(|w[i,j]|^2)# sum of all w squared`\n",
      "- Also can be calculated as `||W||^2 = W.T * W if W is a vector`\n",
      "- Regularization for logistic regression:\n",
      "- The normal cost function that we want to minimize is: `J(w,b) = (1/m) * Sum(L(y(i),y'(i)))`\n",
      "- The L2 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|^2)`\n",
      "- The L1 regularization version: `J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|)`\n",
      "- The L1 regularization version makes a lot of w values become zeros, which makes the model size smaller.\n",
      "- L2 regularization is being used much more often.\n",
      "- `lambda` here is the regularization parameter (hyperparameter)\n",
      "- Regularization for NN:\n",
      "- The normal cost function that we want to minimize is:\n",
      "`J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i)))`  \n",
      "- The L2 regularization version:\n",
      "`J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2)`  \n",
      "- We stack the matrix as one vector `(mn,1)` and then we apply `sqrt(w1^2 + w2^2.....)`  \n",
      "- To do back propagation (old way):\n",
      "`dw[l] = (from back propagation)`  \n",
      "- The new way:\n",
      "`dw[l] = (from back propagation) + lambda/m * w[l]`  \n",
      "- So plugging it in weight update step:  \n",
      "- ```\n",
      "w[l] = w[l] - learning_rate * dw[l]\n",
      "= w[l] - learning_rate * ((from back propagation) + lambda/m * w[l])\n",
      "= w[l] - (learning_rate*lambda/m) * w[l] - learning_rate * (from back propagation)\n",
      "= (1 - (learning_rate*lambda)/m) * w[l] - learning_rate * (from back propagation)\n",
      "```\n",
      "\n",
      "- In practice this penalizes large weights and effectively limits the freedom in your model.\n",
      "\n",
      "- The new term `(1 - (learning_rate*lambda)/m) * w[l]`  causes the **weight to decay** in proportion to its size.\n",
      "\n",
      "\n",
      "### Why regularization reduces overfitting?\n",
      "\n",
      "Here are some intuitions:\n",
      "- Intuition 1:\n",
      "- If `lambda` is too large - a lot of w's will be close to zeros which will make the NN simpler (you can think of it as it would behave closer to logistic regression).\n",
      "- If `lambda` is good enough it will just reduce some weights that makes the neural network overfit.\n",
      "- Intuition 2 (with _tanh_ activation function):\n",
      "- If `lambda` is too large, w's will be small (close to zero) - will use the linear part of the _tanh_ activation function, so we will go from non linear activation to _roughly_ linear which would make the NN a _roughly_ linear classifier.\n",
      "- If `lambda` good enough it will just make some of _tanh_ activations _roughly_ linear which will prevent overfitting.\n",
      "\n",
      "_**Implementation tip**_: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J as a function of the number of iterations of gradient descent and you want to see that the cost function J decreases **monotonically** after every elevation of gradient descent with regularization. If you plot the old definition of J (no regularization) then you might not see it decrease monotonically.\n",
      "\n",
      "\n",
      "### Dropout Regularization\n",
      "\n",
      "- In most cases Andrew Ng tells that he uses the L2 regularization.\n",
      "- The dropout regularization eliminates some neurons/weights on each iteration based on a probability.\n",
      "- A most common technique to implement dropout is called \"Inverted dropout\".\n",
      "- Code for Inverted dropout:\n",
      "\n",
      "```python\n",
      "keep_prob = 0.8   # 0 <= keep_prob <= 1\n",
      "l = 3  # this code is only for layer 3' metadata={'Header 1': 'Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization', 'Header 2': 'Practical aspects of Deep Learning'}\n",
      "12\n",
      "###########################################################\n",
      "page_content='d3 = np.random.rand(a[l].shape[0], a[l].shape[1]) < keep_prob  \n",
      "a3 = np.multiply(a3,d3)   # keep only the values in d3' metadata={'Header 1': 'the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped'}\n",
      "13\n",
      "###########################################################\n",
      "page_content='a3 = a3 / keep_prob\n",
      "```\n",
      "- Vector d[l] is used for forward and back propagation and is the same for them, but it is different for each iteration (pass) or training example.\n",
      "- At test time we don't use dropout. If you implement dropout at test time - it would add noise to predictions.\n",
      "\n",
      "### Understanding Dropout\n",
      "\n",
      "- In the previous video, the intuition was that dropout randomly knocks out units in your network. So it's as if on every iteration you're working with a smaller NN, and so using a smaller NN seems like it should have a regularizing effect.\n",
      "- Another intuition: can't rely on any one feature, so have to spread out weights.\n",
      "- It's possible to show that dropout has a similar effect to L2 regularization.\n",
      "- Dropout can have different `keep_prob` per layer.\n",
      "- The input layer dropout has to be near 1 (or 1 - no dropout) because you don't want to eliminate a lot of features.\n",
      "- If you're more worried about some layers overfitting than others, you can set a lower `keep_prob` for some layers than others. The downside is, this gives you even more hyperparameters to search for using cross-validation. One other alternative might be to have some layers where you apply dropout and some layers where you don't apply dropout and then just have one hyperparameter, which is a `keep_prob` for the layers for which you do apply dropouts.\n",
      "- A lot of researchers are using dropout with Computer Vision (CV) because they have a very big input size and almost never have enough data, so overfitting is the usual problem. And dropout is a regularization technique to prevent overfitting.\n",
      "- A downside of dropout is that the cost function J is not well defined and it will be hard to debug (plot J by iteration).\n",
      "- To solve that you'll need to turn off dropout, set all the `keep_prob`s to 1, and then run the code and check that it monotonically decreases J and then turn on the dropouts again.\n",
      "\n",
      "### Other regularization methods\n",
      "\n",
      "- **Data augmentation**:\n",
      "- For example in a computer vision data:\n",
      "- You can flip all your pictures horizontally this will give you m more data instances.\n",
      "- You could also apply a random position and rotation to an image to get more data.\n",
      "- For example in OCR, you can impose random rotations and distortions to digits/letters.\n",
      "- New data obtained using this technique isn't as good as the real independent data, but still can be used as a regularization technique.\n",
      "- **Early stopping**:\n",
      "- In this technique we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing.\n",
      "- We will pick the point at which the training set error and dev set error are best (lowest training cost with lowest dev cost).\n",
      "- We will take these parameters as the best parameters.\n",
      "- ![](Images/02-_Early_stopping.png)\n",
      "- Andrew prefers to use L2 regularization instead of early stopping because this technique simultaneously tries to minimize the cost function and not to overfit which contradicts the orthogonalization approach (will be discussed further).\n",
      "- But its advantage is that you don't need to search a hyperparameter like in other regularization approaches (like `lambda` in L2 regularization).\n",
      "- **Model Ensembles**:\n",
      "- Algorithm:\n",
      "- Train multiple independent models.\n",
      "- At test time average their results.\n",
      "- It can get you extra 2% performance.\n",
      "- It reduces the generalization error.\n",
      "- You can use some snapshots of your NN at the training ensembles them and take the results.\n",
      "\n",
      "### Normalizing inputs\n",
      "\n",
      "- If you normalize your inputs this will speed up the training process a lot.\n",
      "- Normalization are going on these steps:\n",
      "1. Get the mean of the training set: `mean = (1/m) * sum(x(i))`\n",
      "2. Subtract the mean from each input: `X = X - mean`\n",
      "- This makes your inputs centered around 0.\n",
      "3. Get the variance of the training set: `variance = (1/m) * sum(x(i)^2)`\n",
      "4. Normalize the variance. `X /= variance`\n",
      "- These steps should be applied to training, dev, and testing sets (but using mean and variance of the train set).\n",
      "- Why normalize?\n",
      "- If we don't normalize the inputs our cost function will be deep and its shape will be inconsistent (elongated) then optimizing it will take a long time.\n",
      "- But if we normalize it the opposite will occur. The shape of the cost function will be consistent (look more symmetric like circle in 2D example) and we can use a larger learning rate alpha - the optimization will be faster.\n",
      "\n",
      "### Vanishing / Exploding gradients\n",
      "\n",
      "- The Vanishing / Exploding gradients occurs when your derivatives become very small or very big.\n",
      "- To understand the problem, suppose that we have a deep neural network with number of layers L, and all the activation functions are **linear** and each `b = 0`\n",
      "- Then:\n",
      "```\n",
      "Y' = W[L]W[L-1].....W[2]W[1]X\n",
      "```\n",
      "- Then, if we have 2 hidden units per layer and x1 = x2 = 1, we result in:\n",
      "\n",
      "```\n",
      "if W[l] = [1.5   0]\n",
      "[0   1.5] (l != L because of different dimensions in the output layer)\n",
      "Y' = W[L] [1.5  0]^(L-1) X = 1.5^L # which will be very large\n",
      "[0  1.5]\n",
      "```\n",
      "```\n",
      "if W[l] = [0.5  0]\n",
      "[0  0.5]\n",
      "Y' = W[L] [0.5  0]^(L-1) X = 0.5^L # which will be very small\n",
      "[0  0.5]\n",
      "```\n",
      "- The last example explains that the activations (and similarly derivatives) will be decreased/increased exponentially as a function of number of layers.\n",
      "- So If W > I (Identity matrix) the activation and gradients will explode.\n",
      "- And If W < I (Identity matrix) the activation and gradients will vanish.\n",
      "- Recently Microsoft trained 152 layers (ResNet)! which is a really big number. With such a deep neural network, if your activations or gradients increase or decrease exponentially as a function of L, then these values could get really big or really small. And this makes training difficult, especially if your gradients are exponentially smaller than L, then gradient descent will take tiny little steps. It will take a long time for gradient descent to learn anything.\n",
      "- There is a partial solution that doesn't completely solve this problem but it helps a lot - careful choice of how you initialize the weights (next video).\n",
      "\n",
      "### Weight Initialization for Deep Networks\n",
      "\n",
      "- A partial solution to the Vanishing / Exploding gradients in NN is better or more careful choice of the random initialization of weights\n",
      "- In a single neuron (Perceptron model): `Z = w1x1 + w2x2 + ... + wnxn`\n",
      "- So if `n_x` is large we want `W`'s to be smaller to not explode the cost.\n",
      "- So it turns out that we need the variance which equals `1/n_x` to be the range of `W`'s\n",
      "- So lets say when we initialize `W`'s like this (better to use with `tanh` activation):\n",
      "```\n",
      "np.random.rand(shape) * np.sqrt(1/n[l-1])\n",
      "```\n",
      "or variation of this (Bengio et al.):\n",
      "```\n",
      "np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))\n",
      "```\n",
      "- Setting initialization part inside sqrt to `2/n[l-1]` for `ReLU` is better:\n",
      "```\n",
      "np.random.rand(shape) * np.sqrt(2/n[l-1])\n",
      "```\n",
      "- Number 1 or 2 in the neumerator can also be a hyperparameter to tune (but not the first to start with)\n",
      "- This is one of the best way of partially solution to Vanishing / Exploding gradients (ReLU + Weight Initialization with variance) which will help gradients not to vanish/explode too quickly\n",
      "- The initialization in this video is called \"He Initialization / Xavier Initialization\" and has been published in 2015 paper.\n",
      "\n",
      "### Numerical approximation of gradients\n",
      "\n",
      "- There is an technique called gradient checking which tells you if your implementation of backpropagation is correct.\n",
      "- There's a numerical way to calculate the derivative:\n",
      "![](Images/03-_Numerical_approximation_of_gradients.png)\n",
      "- Gradient checking approximates the gradients and is very helpful for finding the errors in your backpropagation implementation but it's slower than gradient descent (so use only for debugging).\n",
      "- Implementation of this is very simple.\n",
      "- Gradient checking:\n",
      "- First take `W[1],b[1],...,W[L],b[L]` and reshape into one big vector (`theta`)\n",
      "- The cost function will be `J(theta)`\n",
      "- Then take `dW[1],db[1],...,dW[L],db[L]` into one big vector (`d_theta`)\n",
      "- **Algorithm**:\n",
      "```\n",
      "eps = 10^-7   # small number\n",
      "for i in len(theta):\n",
      "d_theta_approx[i] = (J(theta1,...,theta[i] + eps) -  J(theta1,...,theta[i] - eps)) / 2*eps\n",
      "```\n",
      "- Finally we evaluate this formula `(||d_theta_approx - d_theta||) / (||d_theta_approx||+||d_theta||)` (`||` - Euclidean vector norm) and check (with eps = 10^-7):\n",
      "- if it is < 10^-7  - great, very likely the backpropagation implementation is correct\n",
      "- if around 10^-5   - can be OK, but need to inspect if there are no particularly big values in `d_theta_approx - d_theta` vector\n",
      "- if it is >= 10^-3 - bad, probably there is a bug in backpropagation implementation\n",
      "\n",
      "### Gradient checking implementation notes\n",
      "\n",
      "- Don't use the gradient checking algorithm at training time because it's very slow.\n",
      "- Use gradient checking only for debugging.\n",
      "- If algorithm fails grad check, look at components to try to identify the bug.\n",
      "- Don't forget to add `lamda/(2m) * sum(W[l])` to `J` if you are using L1 or L2 regularization.\n",
      "- Gradient checking doesn't work with dropout because J is not consistent.\n",
      "- You can first turn off dropout (set `keep_prob = 1.0`), run gradient checking and then turn on dropout again.\n",
      "- Run gradient checking at random initialization and train the network for a while maybe there's a bug which can be seen when w's and b's become larger (further from 0) and can't be seen on the first iteration (when w's and b's are very small).\n",
      "\n",
      "### Initialization summary\n",
      "\n",
      "- The weights W<sup>[l]</sup> should be initialized randomly to break symmetry\n",
      "\n",
      "- It is however okay to initialize the biases b<sup>[l]</sup> to zeros. Symmetry is still broken so long as W<sup>[l]</sup> is initialized randomly\n",
      "\n",
      "- Different initializations lead to different results\n",
      "\n",
      "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
      "\n",
      "- Don't intialize to values that are too large\n",
      "\n",
      "- He initialization works well for networks with ReLU activations.\n",
      "\n",
      "### Regularization summary\n",
      "\n",
      "#### 1. L2 Regularization\n",
      "**Observations**:\n",
      "- The value of λ is a hyperparameter that you can tune using a dev set.\n",
      "- L2 regularization makes your decision boundary smoother. If λ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
      "\n",
      "**What is L2-regularization actually doing?**:\n",
      "- L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n",
      "\n",
      "**What you should remember:**\n",
      "Implications of L2-regularization on:\n",
      "- cost computation:\n",
      "- A regularization term is added to the cost\n",
      "- backpropagation function:\n",
      "- There are extra terms in the gradients with respect to weight matrices\n",
      "- weights:\n",
      "- weights end up smaller (\"weight decay\") - are pushed to smaller values.\n",
      "\n",
      "#### 2. Dropout\n",
      "**What you should remember about dropout:**\n",
      "- Dropout is a regularization technique.\n",
      "- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
      "- Apply dropout both during forward and backward propagation.\n",
      "- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if `keep_prob` is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.\n",
      "\n",
      "\n",
      "## Optimization algorithms\n",
      "\n",
      "### Mini-batch gradient descent\n",
      "\n",
      "- Training NN with a large data is slow. So to find an optimization algorithm that runs faster is a good idea.\n",
      "- Suppose we have `m = 50 million`. To train this data it will take a huge processing time for one step.\n",
      "- because 50 million won't fit in the memory at once we need other processing to make such a thing.\n",
      "- It turns out you can make a faster algorithm to make gradient descent process some of your items even before you finish the 50 million items.\n",
      "- Suppose we have split m to **mini batches** of size 1000.\n",
      "- `X{1} = 0    ...  1000`\n",
      "- `X{2} = 1001 ...  2000`\n",
      "- `...`\n",
      "- `X{bs} = ...`\n",
      "- We similarly split `X` & `Y`.\n",
      "- So the definition of mini batches ==> `t: X{t}, Y{t}`\n",
      "- In **Batch gradient descent** we run the gradient descent on the whole dataset.\n",
      "- While in **Mini-Batch gradient descent** we run the gradient descent on the mini datasets.\n",
      "- Mini-Batch algorithm pseudo code:\n",
      "```\n",
      "for t = 1:No_of_batches                         # this is called an epoch\n",
      "AL, caches = forward_prop(X{t}, Y{t})\n",
      "cost = compute_cost(AL, Y{t})\n",
      "grads = backward_prop(AL, caches)\n",
      "update_parameters(grads)\n",
      "```\n",
      "- The code inside an epoch should be vectorized.\n",
      "- Mini-batch gradient descent works much faster in the large datasets.\n",
      "\n",
      "### Understanding mini-batch gradient descent\n",
      "\n",
      "- In mini-batch algorithm, the cost won't go down with each step as it does in batch algorithm. It could contain some ups and downs but generally it has to go down (unlike the batch gradient descent where cost function descreases on each iteration).\n",
      "![](Images/04-_batch_vs_mini_batch_cost.png)\n",
      "- Mini-batch size:\n",
      "- (`mini batch size = m`)  ==>    Batch gradient descent\n",
      "- (`mini batch size = 1`)  ==>    Stochastic gradient descent (SGD)\n",
      "- (`mini batch size = between 1 and m`) ==>    Mini-batch gradient descent\n",
      "- Batch gradient descent:\n",
      "- too long per iteration (epoch)\n",
      "- Stochastic gradient descent:\n",
      "- too noisy regarding cost minimization (can be reduced by using smaller learning rate)\n",
      "- won't ever converge (reach the minimum cost)\n",
      "- lose speedup from vectorization\n",
      "- Mini-batch gradient descent:\n",
      "1. faster learning:\n",
      "- you have the vectorization advantage\n",
      "- make progress without waiting to process the entire training set\n",
      "2. doesn't always exactly converge (oscelates in a very small region, but you can reduce learning rate)\n",
      "- Guidelines for choosing mini-batch size:\n",
      "1. If small training set (< 2000 examples) - use batch gradient descent.\n",
      "2. It has to be a power of 2 (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2):\n",
      "`64, 128, 256, 512, 1024, ...`\n",
      "3. Make sure that mini-batch fits in CPU/GPU memory.\n",
      "- Mini-batch size is a `hyperparameter`.\n",
      "\n",
      "### Exponentially weighted averages\n",
      "\n",
      "- There are optimization algorithms that are better than **gradient descent**, but you should first learn about Exponentially weighted averages.\n",
      "- If we have data like the temperature of day through the year it could be like this:\n",
      "```\n",
      "t(1) = 40\n",
      "t(2) = 49\n",
      "t(3) = 45\n",
      "...\n",
      "t(180) = 60\n",
      "...\n",
      "```\n",
      "- This data is small in winter and big in summer. If we plot this data we will find it some noisy.\n",
      "- Now lets compute the Exponentially weighted averages:\n",
      "```\n",
      "V0 = 0\n",
      "V1 = 0.9 * V0 + 0.1 * t(1) = 4# 0.9 and 0.1 are hyperparameters\n",
      "V2 = 0.9 * V1 + 0.1 * t(2) = 8.5\n",
      "V3 = 0.9 * V2 + 0.1 * t(3) = 12.15\n",
      "...\n",
      "```\n",
      "- General equation\n",
      "```\n",
      "V(t) = beta * v(t-1) + (1-beta) * theta(t)\n",
      "```\n",
      "- If we plot this it will represent averages over `~ (1 / (1 - beta))` entries:\n",
      "- `beta = 0.9` will average last 10 entries\n",
      "- `beta = 0.98` will average last 50 entries\n",
      "- `beta = 0.5` will average last 2 entries\n",
      "- Best beta average for our case is between 0.9 and 0.98\n",
      "- **Intuition**: The reason why exponentially weighted averages are useful for further optimizing gradient descent algorithm is that it can give different weights to recent data points (`theta`) based on value of `beta`. If `beta` is high (around 0.9), it smoothens out the averages of skewed data points (oscillations w.r.t. Gradient descent terminology). So this reduces oscillations in gradient descent and hence makes faster and smoother path towerds minima.\n",
      "- Another imagery example:\n",
      "![](Images/Nasdaq1_small.png)\n",
      "_(taken from [investopedia.com](https://www.investopedia.com/))_\n",
      "\n",
      "### Understanding exponentially weighted averages\n",
      "\n",
      "- Intuitions:\n",
      "![](Images/05-_exponentially_weighted_averages_intuitions.png)\n",
      "- We can implement this algorithm with more accurate results using a moving window. But the code is more efficient and faster using the exponentially weighted averages algorithm.\n",
      "- Algorithm is very simple:\n",
      "```\n",
      "v = 0\n",
      "Repeat\n",
      "{\n",
      "Get theta(t)\n",
      "v = beta * v + (1-beta) * theta(t)\n",
      "}\n",
      "```\n",
      "\n",
      "### Bias correction in exponentially weighted averages\n",
      "\n",
      "- The bias correction helps make the exponentially weighted averages more accurate.\n",
      "- Because `v(0) = 0`, the bias of the weighted averages is shifted and the accuracy suffers at the start.\n",
      "- To solve the bias issue we have to use this equation:\n",
      "```\n",
      "v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)\n",
      "```\n",
      "- As t becomes larger the `(1 - beta^t)` becomes close to `1`\n",
      "\n",
      "### Gradient descent with momentum\n",
      "\n",
      "- The momentum algorithm almost always works faster than standard gradient descent.\n",
      "- The simple idea is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.\n",
      "- Pseudo code:\n",
      "```\n",
      "vdW = 0, vdb = 0\n",
      "on iteration t:' metadata={'Header 1': '(ensures that the expected value of a3 remains the same) - to solve the scaling problem'}\n",
      "14\n",
      "###########################################################\n",
      "page_content='compute dw, db on current mini-batch  \n",
      "vdW = beta * vdW + (1 - beta) * dW\n",
      "vdb = beta * vdb + (1 - beta) * db\n",
      "W = W - learning_rate * vdW\n",
      "b = b - learning_rate * vdb\n",
      "```\n",
      "- Momentum helps the cost function to go to the minimum point in a more fast and consistent way.\n",
      "- `beta` is another `hyperparameter`. `beta = 0.9` is very common and works very well in most cases.\n",
      "- In practice people don't bother implementing **bias correction**.\n",
      "\n",
      "### RMSprop\n",
      "\n",
      "- Stands for **Root mean square prop**.\n",
      "- This algorithm speeds up the gradient descent.\n",
      "- Pseudo code:\n",
      "```\n",
      "sdW = 0, sdb = 0\n",
      "on iteration t:  \n",
      "compute dw, db on current mini-batch  \n",
      "sdW = (beta * sdW) + (1 - beta) * dW^2  # squaring is element-wise\n",
      "sdb = (beta * sdb) + (1 - beta) * db^2  # squaring is element-wise\n",
      "W = W - learning_rate * dW / sqrt(sdW)\n",
      "b = B - learning_rate * db / sqrt(sdb)\n",
      "```\n",
      "- RMSprop will make the cost function move slower on the vertical direction and faster on the horizontal direction in the following example:\n",
      "![](Images/06-_RMSprop.png)\n",
      "- Ensure that `sdW` is not zero by adding a small value `epsilon` (e.g. `epsilon = 10^-8`) to it:\n",
      "`W = W - learning_rate * dW / (sqrt(sdW) + epsilon)`\n",
      "- With RMSprop you can increase your learning rate.\n",
      "- Developed by Geoffrey Hinton and firstly introduced on [Coursera.org](https://www.coursera.org/) course.\n",
      "\n",
      "### Adam optimization algorithm\n",
      "\n",
      "- Stands for **Adaptive Moment Estimation**.\n",
      "- Adam optimization and RMSprop are among the optimization algorithms that worked very well with a lot of NN architectures.\n",
      "- Adam optimization simply puts RMSprop and momentum together!\n",
      "- Pseudo code:\n",
      "```\n",
      "vdW = 0, vdW = 0\n",
      "sdW = 0, sdb = 0\n",
      "on iteration t:  \n",
      "compute dw, db on current mini-batch  \n",
      "vdW = (beta1 * vdW) + (1 - beta1) * dW     # momentum\n",
      "vdb = (beta1 * vdb) + (1 - beta1) * db     # momentum  \n",
      "sdW = (beta2 * sdW) + (1 - beta2) * dW^2   # RMSprop\n",
      "sdb = (beta2 * sdb) + (1 - beta2) * db^2   # RMSprop  \n",
      "vdW = vdW / (1 - beta1^t)      # fixing bias\n",
      "vdb = vdb / (1 - beta1^t)      # fixing bias  \n",
      "sdW = sdW / (1 - beta2^t)      # fixing bias\n",
      "sdb = sdb / (1 - beta2^t)      # fixing bias  \n",
      "W = W - learning_rate * vdW / (sqrt(sdW) + epsilon)\n",
      "b = B - learning_rate * vdb / (sqrt(sdb) + epsilon)\n",
      "```\n",
      "- Hyperparameters for Adam:\n",
      "- Learning rate: needed to be tuned.\n",
      "- `beta1`: parameter of the momentum - `0.9` is recommended by default.\n",
      "- `beta2`: parameter of the RMSprop - `0.999` is recommended by default.\n",
      "- `epsilon`: `10^-8` is recommended by default.\n",
      "\n",
      "### Learning rate decay\n",
      "\n",
      "- Slowly reduce learning rate.\n",
      "- As mentioned before mini-batch gradient descent won't reach the optimum point (converge). But by making the learning rate decay with iterations it will be much closer to it because the steps (and possible oscillations) near the optimum are smaller.\n",
      "- One technique equations is`learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate_0`\n",
      "- `epoch_num` is over all data (not a single mini-batch).\n",
      "- Other learning rate decay methods (continuous):\n",
      "- `learning_rate = (0.95 ^ epoch_num) * learning_rate_0`\n",
      "- `learning_rate = (k / sqrt(epoch_num)) * learning_rate_0`\n",
      "- Some people perform learning rate decay discretely - repeatedly decrease after some number of epochs.\n",
      "- Some people are making changes to the learning rate manually.\n",
      "- `decay_rate` is another `hyperparameter`.\n",
      "- For Andrew Ng, learning rate decay has less priority.\n",
      "\n",
      "### The problem of local optima\n",
      "\n",
      "- The normal local optima is not likely to appear in a deep neural network because data is usually high dimensional. For point to be a local optima it has to be a local optima for each of the dimensions which is highly unlikely.\n",
      "- It's unlikely to get stuck in a bad local optima in high dimensions, it is much more likely to get to the saddle point rather to the local optima, which is not a problem.\n",
      "- Plateaus can make learning slow:\n",
      "- Plateau is a region where the derivative is close to zero for a long time.\n",
      "- This is where algorithms like momentum, RMSprop or Adam can help.\n",
      "\n",
      "\n",
      "\n",
      "## Hyperparameter tuning, Batch Normalization and Programming Frameworks\n",
      "\n",
      "### Tuning process\n",
      "\n",
      "- We need to tune our hyperparameters to get the best out of them.\n",
      "- Hyperparameters importance are (as for Andrew Ng):\n",
      "1. Learning rate.\n",
      "2. Momentum beta.\n",
      "3. Mini-batch size.\n",
      "4. No. of hidden units.\n",
      "5. No. of layers.\n",
      "6. Learning rate decay.\n",
      "7. Regularization lambda.\n",
      "8. Activation functions.\n",
      "9. Adam `beta1`, `beta2` & `epsilon`.\n",
      "- Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\n",
      "- One of the ways to tune is to sample a grid with `N` hyperparameter settings and then try all settings combinations on your problem.\n",
      "- Try random values: don't use a grid.\n",
      "- You can use `Coarse to fine sampling scheme`:\n",
      "- When you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.\n",
      "- These methods can be automated.\n",
      "\n",
      "### Using an appropriate scale to pick hyperparameters\n",
      "\n",
      "- Let's say you have a specific range for a hyperparameter from \"a\" to \"b\". It's better to search for the right ones using the logarithmic scale rather then in linear scale:\n",
      "- Calculate: `a_log = log(a)  # e.g. a = 0.0001 then a_log = -4`\n",
      "- Calculate: `b_log = log(b)  # e.g. b = 1  then b_log = 0`\n",
      "- Then:\n",
      "```\n",
      "r = (a_log - b_log) * np.random.rand() + b_log' metadata={'Header 1': 'can be mini-batch or batch gradient descent'}\n",
      "15\n",
      "###########################################################\n",
      "page_content='result = 10^r\n",
      "```\n",
      "It uniformly samples values in log scale from [a,b].\n",
      "- If we want to use the last method on exploring on the \"momentum beta\":\n",
      "- Beta best range is from 0.9 to 0.999.\n",
      "- You should search for `1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999)` and the use `a = 0.001` and `b = 0.1`. Then:\n",
      "```\n",
      "a_log = -3\n",
      "b_log = -1\n",
      "r = (a_log - b_log) * np.random.rand() + b_log\n",
      "beta = 1 - 10^r   # because 1 - beta = 10^r\n",
      "```\n",
      "\n",
      "### Hyperparameters tuning in practice: Pandas vs. Caviar\n",
      "\n",
      "- Intuitions about hyperparameter settings from one application area may or may not transfer to a different one.\n",
      "- If you don't have much computational resources you can use the \"babysitting model\":\n",
      "- Day 0 you might initialize your parameter as random and then start training.\n",
      "- Then you watch your learning curve gradually decrease over the day.\n",
      "- And each day you nudge your parameters a little during training.\n",
      "- Called panda approach.\n",
      "- If you have enough computational resources, you can run some models in parallel and at the end of the day(s) you check the results.\n",
      "- Called Caviar approach.\n",
      "\n",
      "### Normalizing activations in a network\n",
      "\n",
      "- In the rise of deep learning, one of the most important ideas has been an algorithm called **batch normalization**, created by two researchers, Sergey Ioffe and Christian Szegedy.\n",
      "- Batch Normalization speeds up learning.\n",
      "- Before we normalized input by subtracting the mean and dividing by variance. This helped a lot for the shape of the cost function and for reaching the minimum point faster.\n",
      "- The question is: *for any hidden layer can we normalize `A[l]` to train `W[l+1]`, `b[l+1]` faster?* This is what batch normalization is about.\n",
      "- There are some debates in the deep learning literature about whether you should normalize values before the activation function `Z[l]` or after applying the activation function `A[l]`. In practice, normalizing `Z[l]` is done much more often and that is what Andrew Ng presents.\n",
      "- Algorithm:\n",
      "- Given `Z[l] = [z(1), ..., z(m)]`, i = 1 to m (for each input)\n",
      "- Compute `mean = 1/m * sum(z[i])`\n",
      "- Compute `variance = 1/m * sum((z[i] - mean)^2)`\n",
      "- Then `Z_norm[i] = (z[i] - mean) / np.sqrt(variance + epsilon)` (add `epsilon` for numerical stability if variance = 0)\n",
      "- Forcing the inputs to a distribution with zero mean and variance of 1.\n",
      "- Then `Z_tilde[i] = gamma * Z_norm[i] + beta`\n",
      "- To make inputs belong to other distribution (with other mean and variance).\n",
      "- gamma and beta are learnable parameters of the model.\n",
      "- Making the NN learn the distribution of the outputs.\n",
      "- _Note:_ if `gamma = sqrt(variance + epsilon)` and `beta = mean` then `Z_tilde[i] = z[i]`\n",
      "\n",
      "### Fitting Batch Normalization into a neural network\n",
      "\n",
      "- Using batch norm in 3 hidden layers NN:\n",
      "![](Images/bn.png)\n",
      "- Our NN parameters will be:\n",
      "- `W[1]`, `b[1]`, ..., `W[L]`, `b[L]`, `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]`\n",
      "- `beta[1]`, `gamma[1]`, ..., `beta[L]`, `gamma[L]` are updated using any optimization algorithms (like GD, RMSprop, Adam)\n",
      "- If you are using a deep learning framework, you won't have to implement batch norm yourself:\n",
      "- Ex. in Tensorflow you can add this line: `tf.nn.batch-normalization()`\n",
      "- Batch normalization is usually applied with mini-batches.\n",
      "- If we are using batch normalization parameters `b[1]`, ..., `b[L]` doesn't count because they will be eliminated after mean subtraction step, so:\n",
      "```\n",
      "Z[l] = W[l]A[l-1] + b[l] => Z[l] = W[l]A[l-1]\n",
      "Z_norm[l] = ...\n",
      "Z_tilde[l] = gamma[l] * Z_norm[l] + beta[l]\n",
      "```\n",
      "- Taking the mean of a constant `b[l]` will eliminate the `b[l]`\n",
      "- So if you are using batch normalization, you can remove b[l] or make it always zero.\n",
      "- So the parameters will be `W[l]`, `beta[l]`, and `alpha[l]`.\n",
      "- Shapes:\n",
      "- `Z[l]       - (n[l], m)`\n",
      "- `beta[l]    - (n[l], m)`\n",
      "- `gamma[l]   - (n[l], m)`\n",
      "\n",
      "### Why does Batch normalization work?\n",
      "\n",
      "- The first reason is the same reason as why we normalize X.\n",
      "- The second reason is that batch normalization reduces the problem of input values changing (shifting).\n",
      "- Batch normalization does some regularization:\n",
      "- Each mini batch is scaled by the mean/variance computed of that mini-batch.\n",
      "- This adds some noise to the values `Z[l]` within that mini batch. So similar to dropout it adds some noise to each hidden layer's activations.\n",
      "- This has a slight regularization effect.\n",
      "- Using bigger size of the mini-batch you are reducing noise and therefore regularization effect.\n",
      "- Don't rely on batch normalization as a regularization. It's intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques (L2 or dropout).\n",
      "\n",
      "### Batch normalization at test time\n",
      "\n",
      "- When we train a NN with Batch normalization, we compute the mean and the variance of the mini-batch.\n",
      "- In testing we might need to process examples one at a time. The mean and the variance of one example won't make sense.\n",
      "- We have to compute an estimated value of mean and variance to use it in testing time.\n",
      "- We can use the weighted average across the mini-batches.\n",
      "- We will use the estimated values of the mean and variance to test.\n",
      "- This method is also sometimes called \"Running average\".\n",
      "- In practice most often you will use a deep learning framework and it will contain some default implementation of doing such a thing.\n",
      "\n",
      "### Softmax Regression\n",
      "\n",
      "- In every example we have used so far we were talking about binary classification.\n",
      "- There are a generalization of logistic regression called Softmax regression that is used for multiclass classification/regression.\n",
      "- For example if we are classifying by classes `dog`, `cat`, `baby chick` and `none of that`\n",
      "- Dog `class = 1`\n",
      "- Cat `class = 2`\n",
      "- Baby chick `class = 3`\n",
      "- None `class = 0`\n",
      "- To represent a dog vector `y = [0 1 0 0]`\n",
      "- To represent a cat vector `y = [0 0 1 0]`\n",
      "- To represent a baby chick vector `y = [0 0 0 1]`\n",
      "- To represent a none vector `y = [1 0 0 0]`\n",
      "- Notations:\n",
      "- `C = no. of classes`\n",
      "- Range of classes is `(0, ..., C-1)`\n",
      "- In output layer `Ny = C`\n",
      "- Each of C values in the output layer will contain a probability of the example to belong to each of the classes.\n",
      "- In the last layer we will have to activate the Softmax activation function instead of the sigmoid activation.\n",
      "- Softmax activation equations:\n",
      "```\n",
      "t = e^(Z[L])                      # shape(C, m)\n",
      "A[L] = e^(Z[L]) / sum(t)          # shape(C, m), sum(t) - sum of t's for each example (shape (1, m))\n",
      "```\n",
      "\n",
      "### Training a Softmax classifier\n",
      "\n",
      "- There's an activation which is called hard max, which gets 1 for the maximum value and zeros for the others.\n",
      "- If you are using NumPy, its `np.max` over the vertical axis.\n",
      "- The Softmax name came from softening the values and not harding them like hard max.\n",
      "- Softmax is a generalization of logistic activation function to `C` classes. If `C = 2` softmax reduces to logistic regression.\n",
      "- The loss function used with softmax:\n",
      "```\n",
      "L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1\n",
      "```\n",
      "- The cost function used with softmax:\n",
      "```\n",
      "J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m\n",
      "```\n",
      "- Back propagation with softmax:\n",
      "```\n",
      "dZ[L] = Y_hat - Y\n",
      "```\n",
      "- The derivative of softmax is:\n",
      "```\n",
      "Y_hat * (1 - Y_hat)\n",
      "```\n",
      "- Example:\n",
      "![](Images/07-_softmax.png)\n",
      "\n",
      "### Deep learning frameworks\n",
      "\n",
      "- It's not practical to implement everything from scratch. Our numpy implementations were to know how NN works.\n",
      "- There are many good deep learning frameworks.\n",
      "- Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on going.\n",
      "- Here are some of the leading deep learning frameworks:\n",
      "- Caffe/ Caffe2\n",
      "- CNTK\n",
      "- DL4j\n",
      "- Keras\n",
      "- Lasagne\n",
      "- mxnet\n",
      "- PaddlePaddle\n",
      "- TensorFlow\n",
      "- Theano\n",
      "- Torch/Pytorch\n",
      "- These frameworks are getting better month by month. Comparison between them can be found [here](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
      "- How to choose deep learning framework:\n",
      "- Ease of programming (development and deployment)\n",
      "- Running speed\n",
      "- Truly open (open source with good governance)\n",
      "- Programming frameworks can not only shorten your coding time but sometimes also perform optimizations that speed up your code.\n",
      "\n",
      "### TensorFlow\n",
      "\n",
      "- In this section we will learn the basic structure of TensorFlow programs.\n",
      "- Lets see how to implement a minimization function:\n",
      "- Example function: `J(w) = w^2 - 10w + 25`\n",
      "- The result should be `w = 5` as the function is `(w-5)^2 = 0`\n",
      "- Code v.1:\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf  \n",
      "w = tf.Variable(0, dtype=tf.float32)                 # creating a variable w\n",
      "cost = tf.add(tf.add(w**2, tf.multiply(-10.0, w)), 25.0)        # can be written as this - cost = w**2 - 10*w + 25\n",
      "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  \n",
      "init = tf.global_variables_initializer()\n",
      "session = tf.Session()\n",
      "session.run(init)\n",
      "session.run(w)    # Runs the definition of w, if you print this it will print zero\n",
      "session.run(train)  \n",
      "print(\"W after one iteration:\", session.run(w))  \n",
      "for i in range(1000):\n",
      "session.run(train)  \n",
      "print(\"W after 1000 iterations:\", session.run(w))\n",
      "```\n",
      "- Code v.2 (we feed the inputs to the algorithm through coefficients):\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf  \n",
      "coefficients = np.array([[1.], [-10.], [25.]])  \n",
      "x = tf.placeholder(tf.float32, [3, 1])\n",
      "w = tf.Variable(0, dtype=tf.float32)                 # Creating a variable w\n",
      "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]  \n",
      "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  \n",
      "init = tf.global_variables_initializer()\n",
      "session = tf.Session()\n",
      "session.run(init)\n",
      "session.run(w)    # Runs the definition of w, if you print this it will print zero\n",
      "session.run(train, feed_dict={x: coefficients})  \n",
      "print(\"W after one iteration:\", session.run(w))  \n",
      "for i in range(1000):\n",
      "session.run(train, feed_dict={x: coefficients})  \n",
      "print(\"W after 1000 iterations:\", session.run(w))\n",
      "```\n",
      "- In TensorFlow you implement only the forward propagation and TensorFlow will do the backpropagation by itself.\n",
      "- In TensorFlow a placeholder is a variable you can assign a value to later.\n",
      "- If you are using a mini-batch training you should change the `feed_dict={x: coefficients}` to the current mini-batch data.\n",
      "- Almost all TensorFlow programs use this:\n",
      "```python\n",
      "with tf.Session() as session:       # better for cleaning up in case of error/exception\n",
      "session.run(init)\n",
      "session.run(w)\n",
      "```\n",
      "- In deep learning frameworks there are a lot of things that you can do with one line of code like changing the optimizer.\n",
      "_**Side notes:**_\n",
      "- Writing and running programs in TensorFlow has the following steps:\n",
      "1. Create Tensors (variables) that are not yet executed/evaluated.\n",
      "2. Write operations between those Tensors.\n",
      "3. Initialize your Tensors.\n",
      "4. Create a Session.\n",
      "5. Run the Session. This will run the operations you'd written above.\n",
      "- Instead of needing to write code to compute the cost function we know, we can use this line in TensorFlow :\n",
      "`tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
      "- To initialize weights in NN using TensorFlow use:\n",
      "```\n",
      "W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))  \n",
      "b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
      "```\n",
      "- For 3-layer NN, it is important to note that the forward propagation stops at `Z3`. The reason is that in TensorFlow the last linear layer output is given as input to the function computing the loss. Therefore, you don't need `A3`!\n",
      "- To reset the graph use `tf.reset_default_graph()`\n",
      "\n",
      "## Extra Notes\n",
      "\n",
      "- If you want a good papers in deep learning look at the ICLR proceedings (Or NIPS proceedings) and that will give you a really good view of the field.\n",
      "- Who is Yuanqing Lin?\n",
      "- Head of Baidu research.\n",
      "- First one to win ImageNet\n",
      "- Works in PaddlePaddle deep learning platform.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<br><br>\n",
      "<br><br>\n",
      "These Notes were made by [Mahmoud Badry](mailto:mma18@fayoum.edu.eg) @2017\n",
      "# Deep Learning Specialization on Coursera (offered by deeplearning.ai)\n",
      "\n",
      "Programming assignments and quizzes from all courses in the Coursera [Deep Learning specialization](https://www.coursera.org/specializations/deep-learning) offered by `deeplearning.ai`.\n",
      "\n",
      "Instructor: [Andrew Ng](http://www.andrewng.org/)\n",
      "\n",
      "## Notes\n",
      "\n",
      "### For detailed interview-ready notes on all courses in the Coursera Deep Learning specialization, refer [www.aman.ai](https://aman.ai/).\n",
      "\n",
      "## Setup\n",
      "\n",
      "Run ```setup.sh``` to (i) download a pre-trained VGG-19 dataset and (ii) extract the zip'd pre-trained models and datasets that are needed for all the assignments.\n",
      "\n",
      "## Credits\n",
      "\n",
      "This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning), unless specified otherwise.\n",
      "\n",
      "## 2021 Version\n",
      "\n",
      "This specialization was updated in April 2021 to include developments in deep learning and programming frameworks, with the biggest change being shifting from TensorFlow 1 to TensorFlow 2. This repo has been updated accordingly as well.\n",
      "\n",
      "## Programming Assignments\n",
      "\n",
      "### Course 1: Neural Networks and Deep Learning\n",
      "\n",
      "- [Week 2 - PA 1 - Python Basics with Numpy](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb)\n",
      "- [Week 2 - PA 2 - Logistic Regression with a Neural Network mindset](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb)\n",
      "- [Week 3 - PA 3 - Planar data classification with one hidden layer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb)\n",
      "- [Week 4 - PA 4 - Building your Deep Neural Network: Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb)\n",
      "- [Week 4 - PA 5 - Deep Neural Network for Image Classification: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb)\n",
      "\n",
      "### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n",
      "\n",
      "- [Week 1 - PA 1 - Initialization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb)\n",
      "- [Week 1 - PA 2 - Regularization](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb)\n",
      "- [Week 1 - PA 3 - Gradient Checking](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb)\n",
      "- [Week 2 - PA 4 - Optimization Methods](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb)\n",
      "- [Week 3 - PA 5 - TensorFlow Tutorial](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Tensorflow_introduction.ipynb)\n",
      "\n",
      "### Course 3: Structuring Machine Learning Projects\n",
      "\n",
      "- There are no programming assignments for this course. But this course comes with very interesting case study quizzes (below).\n",
      "\n",
      "### Course 4: Convolutional Neural Networks\n",
      "\n",
      "- [Week 1 - PA 1 - Convolutional Model: step by step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v1.ipynb)\n",
      "- [Week 1 - PA 2 - Convolutional Neural Networks: Application](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application.ipynb)\n",
      "- [Week 2 - PA 1 - Keras - Tutorial - Happy House](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb)\n",
      "- [Week 2 - PA 2 - Residual Networks](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks.ipynb)\n",
      "- [Week 2 - PA 2 - Transfer Learning with MobileNet](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Transfer%20Learning%20with%20MobileNet/Transfer_learning_with_MobileNet_v1.ipynb)\n",
      "- [Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection.ipynb)\n",
      "- [Week 3 - PA 2 - Image Segmentation Unet](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Image%20Segmentation%20Unet/Image_segmentation_Unet_v2.ipynb)\n",
      "- [Week 4 - PA 1 - Art Generation with Neural Style Transfer](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb)\n",
      "- [Week 4 - PA 2 - Face Recognition](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition.ipynb)\n",
      "\n",
      "### Course 5: Sequence Models\n",
      "\n",
      "- [Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb)\n",
      "- [Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model.ipynb)\n",
      "- [Week 1 - PA 3 - Jazz improvisation with LSTM](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4_Solution.ipynb)\n",
      "- [Week 2 - PA 1 - Word Vector Representation and Debiasing](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb)\n",
      "- [Week 2 - PA 2 - Emojify!](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emoji_v3a.ipynb)\n",
      "- [Week 3 - PA 1 - Neural Machine Translation with Attention](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb)\n",
      "- [Week 3 - PA 2 - Trigger Word Detection](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v2a.ipynb)\n",
      "- [Week 4 - PA 1 - Transformer Network](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%204/Transformer%20Subclass/C5_W4_A1_Transformer_Subclass_v1.ipynb)\n",
      "- [Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Named%20Entity%20Recognition/Transformer_application_Named_Entity_Recognition.ipynb)\n",
      "- [Week 3 - PA 2 - Transformer Network Application: Question Answering](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Question%20Answering/QA_transformer.ipynb)\n",
      "\n",
      "## Quiz Solutions\n",
      "\n",
      "### Course 1: Neural Networks and Deep Learning\n",
      "\n",
      "- Week 1 Quiz - Introduction to deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf)\n",
      "- Week 2 Quiz - Neural Network Basics: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf)\n",
      "- Week 3 Quiz - Shallow Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf)\n",
      "- Week 4 Quiz - Key concepts on Deep Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf)\n",
      "\n",
      "### Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n",
      "\n",
      "- Week 1 Quiz - Practical aspects of deep learning: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf)\n",
      "- Week 2 Quiz - Optimization algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf)\n",
      "- Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf)\n",
      "\n",
      "### Course 3: Structuring Machine Learning Projects\n",
      "\n",
      "- Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf)\n",
      "- Week 2 Quiz - Autonomous driving (case study): [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf)\n",
      "\n",
      "### Course 4: Convolutional Neural Networks\n",
      "\n",
      "- Week 1 Quiz - The basics of ConvNets: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf)\n",
      "- Week 2 Quiz - Deep convolutional models: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf)\n",
      "- Week 3 Quiz - Detection algorithms: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf)\n",
      "- Week 4 Quiz - Special applications: Face recognition & Neural style transfer: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf)\n",
      "\n",
      "### Course 5: Sequence Models\n",
      "\n",
      "- Week 1 Quiz - Recurrent Neural Networks: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf)\n",
      "- Week 2 Quiz - Natural Language Processing & Word Embeddings: [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf)\n",
      "- Week 3 Quiz - Sequence models & Attention mechanism: [Text](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md) | [PDF](https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf)\n",
      "\n",
      "## Disclaimer\n",
      "\n",
      "I recognize the time people spend on building intuition, understanding new concepts and debugging assignments. The solutions uploaded here are **only for reference**. They are meant to unblock you if you get stuck somewhere. Please do not copy any part of the code as-is (the programming assignments are fairly easy if you read the instructions carefully). Similarly, try out the quizzes yourself before you refer to the quiz solutions. This course is the most straight-forward deep learning course I have ever taken, with fabulous course content and structure. It's a treasure by the deeplearning.ai team.\n",
      "' metadata={'Header 1': 'In the example the range would be from [-4, 0] because rand range [0,1)'}\n"
     ]
    }
   ],
   "source": [
    "for index, splitted_markdown in enumerate(md_header_splits):\n",
    "    print(index)\n",
    "    print(\"###########################################################\")\n",
    "    print(splitted_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
